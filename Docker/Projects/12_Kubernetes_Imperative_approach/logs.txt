
==> Audit <==
|-----------|-----------------------|----------|-------------|---------|----------------------|----------------------|
|  Command  |         Args          | Profile  |    User     | Version |      Start Time      |       End Time       |
|-----------|-----------------------|----------|-------------|---------|----------------------|----------------------|
| start     | --driver=virtualbox   | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:04 EEST |                      |
| delete    |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:07 EEST | 16 May 24 10:07 EEST |
| start     | --driver=docker       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:15 EEST | 16 May 24 10:27 EEST |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:29 EEST |                      |
| addons    | enable metrics-server | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:31 EEST | 16 May 24 10:31 EEST |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:31 EEST |                      |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:33 EEST |                      |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:36 EEST |                      |
| dashboard | --url                 | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:39 EEST |                      |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:41 EEST |                      |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 10:56 EEST |                      |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 11:04 EEST |                      |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 11:10 EEST |                      |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 11:15 EEST |                      |
| addons    | enable dashboard      | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 11:17 EEST | 16 May 24 11:17 EEST |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 11:17 EEST |                      |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 16 May 24 11:20 EEST |                      |
| start     | --driver=docker       | minikube | ZBOOK\max20 | v1.33.0 | 17 May 24 10:47 EEST | 17 May 24 10:48 EEST |
| start     | --driver=docker       | minikube | ZBOOK\max20 | v1.33.0 | 18 May 24 11:29 EEST | 18 May 24 11:29 EEST |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 18 May 24 11:34 EEST |                      |
| service   | first-app             | minikube | ZBOOK\max20 | v1.33.0 | 18 May 24 12:03 EEST | 18 May 24 16:08 EEST |
| dashboard |                       | minikube | ZBOOK\max20 | v1.33.0 | 18 May 24 14:41 EEST |                      |
| service   | backend               | minikube | ZBOOK\max20 | v1.33.0 | 18 May 24 15:58 EEST | 18 May 24 16:10 EEST |
| service   | backend               | minikube | ZBOOK\max20 | v1.33.0 | 18 May 24 16:15 EEST |                      |
|-----------|-----------------------|----------|-------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2024/05/18 11:29:21
Running on machine: ZBOOK
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0518 11:29:21.111514   23392 out.go:291] Setting OutFile to fd 180 ...
I0518 11:29:21.111514   23392 out.go:343] isatty.IsTerminal(180) = true
I0518 11:29:21.111514   23392 out.go:304] Setting ErrFile to fd 184...
I0518 11:29:21.111514   23392 out.go:343] isatty.IsTerminal(184) = true
I0518 11:29:21.175650   23392 out.go:298] Setting JSON to false
I0518 11:29:21.188493   23392 start.go:129] hostinfo: {"hostname":"ZBOOK","uptime":2322,"bootTime":1716018639,"procs":374,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.3447 Build 22631.3447","kernelVersion":"10.0.22631.3447 Build 22631.3447","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"55b00c50-f9df-4c7a-ba69-46c5e93b4871"}
W0518 11:29:21.188493   23392 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0518 11:29:21.190091   23392 out.go:177] üòÑ  minikube v1.33.0 –Ω–∞ Microsoft Windows 11 Home 10.0.22631.3447 Build 22631.3447
I0518 11:29:21.192224   23392 notify.go:220] Checking for updates...
I0518 11:29:21.206510   23392 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0518 11:29:21.223999   23392 driver.go:392] Setting default libvirt URI to qemu:///system
I0518 11:29:21.359557   23392 docker.go:122] docker version: linux-25.0.3:Docker Desktop 4.27.2 (137060)
I0518 11:29:21.363177   23392 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0518 11:29:21.504278   23392 lock.go:35] WriteFile acquiring C:\Users\max20\.minikube\last_update_check: {Name:mk5d43a5823790e1822446f09c107a76eaae322d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 11:29:21.513358   23392 out.go:177] üéâ  minikube 1.33.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.33.1
I0518 11:29:21.518033   23392 out.go:177] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0518 11:29:21.697580   23392 info.go:266] docker info: {ID:2716cc0e-2bfc-4ada-888f-32f09ed51618 Containers:14 ContainersRunning:0 ContainersPaused:0 ContainersStopped:14 Images:13 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:44 OomKillDisable:true NGoroutines:75 SystemTime:2024-05-18 08:23:36.150600624 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16655556608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.5-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.4.1]] Warnings:<nil>}}
I0518 11:29:21.698090   23392 out.go:177] ‚ú®  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä–∞–π–≤–µ—Ä docker –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
I0518 11:29:21.699637   23392 start.go:297] selected driver: docker
I0518 11:29:21.699637   23392 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\max20:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0518 11:29:21.699637   23392 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0518 11:29:21.705271   23392 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0518 11:29:22.022107   23392 info.go:266] docker info: {ID:2716cc0e-2bfc-4ada-888f-32f09ed51618 Containers:14 ContainersRunning:0 ContainersPaused:0 ContainersStopped:14 Images:13 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:44 OomKillDisable:true NGoroutines:75 SystemTime:2024-05-18 08:23:36.150600624 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16655556608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.5-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.4.1]] Warnings:<nil>}}
I0518 11:29:22.097783   23392 cni.go:84] Creating CNI manager for ""
I0518 11:29:22.098287   23392 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0518 11:29:22.098287   23392 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\max20:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0518 11:29:22.098830   23392 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0518 11:29:22.100387   23392 cache.go:121] Beginning downloading kic base image for docker with docker
I0518 11:29:22.100387   23392 out.go:177] üöú  Pulling base image v0.0.43 ...
I0518 11:29:22.101432   23392 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0518 11:29:22.101432   23392 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0518 11:29:22.101954   23392 preload.go:147] Found local preload: C:\Users\max20\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0518 11:29:22.101954   23392 cache.go:56] Caching tarball of preloaded images
I0518 11:29:22.101954   23392 preload.go:173] Found C:\Users\max20\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0518 11:29:22.101954   23392 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0518 11:29:22.101954   23392 profile.go:143] Saving config to C:\Users\max20\.minikube\profiles\minikube\config.json ...
I0518 11:29:22.243471   23392 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0518 11:29:22.243471   23392 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0518 11:29:22.244014   23392 cache.go:194] Successfully downloaded all kic artifacts
I0518 11:29:22.245061   23392 start.go:360] acquireMachinesLock for minikube: {Name:mke8ef0096cd21c991caa834c18c87e497867c2c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0518 11:29:22.245061   23392 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0518 11:29:22.245061   23392 start.go:96] Skipping create...Using existing machine configuration
I0518 11:29:22.245061   23392 fix.go:54] fixHost starting: 
I0518 11:29:22.251014   23392 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 11:29:22.354577   23392 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0518 11:29:22.354577   23392 fix.go:138] unexpected machine state, will restart: <nil>
I0518 11:29:22.355624   23392 out.go:177] üîÑ  –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π docker container –¥–ª—è "minikube" ...
I0518 11:29:22.359750   23392 cli_runner.go:164] Run: docker start minikube
I0518 11:29:22.980081   23392 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 11:29:23.177423   23392 kic.go:430] container "minikube" state is running.
I0518 11:29:23.184997   23392 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0518 11:29:23.362374   23392 profile.go:143] Saving config to C:\Users\max20\.minikube\profiles\minikube\config.json ...
I0518 11:29:23.364018   23392 machine.go:94] provisionDockerMachine start ...
I0518 11:29:23.368847   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:23.542789   23392 main.go:141] libmachine: Using SSH client type: native
I0518 11:29:23.554101   23392 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd3a1c0] 0xd3cda0 <nil>  [] 0s} 127.0.0.1 63147 <nil> <nil>}
I0518 11:29:23.554101   23392 main.go:141] libmachine: About to run SSH command:
hostname
I0518 11:29:23.703754   23392 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0518 11:29:23.703824   23392 ubuntu.go:169] provisioning hostname "minikube"
I0518 11:29:23.709151   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:23.858646   23392 main.go:141] libmachine: Using SSH client type: native
I0518 11:29:23.858646   23392 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd3a1c0] 0xd3cda0 <nil>  [] 0s} 127.0.0.1 63147 <nil> <nil>}
I0518 11:29:23.858646   23392 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0518 11:29:24.010545   23392 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0518 11:29:24.014246   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:24.173039   23392 main.go:141] libmachine: Using SSH client type: native
I0518 11:29:24.173552   23392 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd3a1c0] 0xd3cda0 <nil>  [] 0s} 127.0.0.1 63147 <nil> <nil>}
I0518 11:29:24.173552   23392 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0518 11:29:24.311499   23392 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0518 11:29:24.311499   23392 ubuntu.go:175] set auth options {CertDir:C:\Users\max20\.minikube CaCertPath:C:\Users\max20\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\max20\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\max20\.minikube\machines\server.pem ServerKeyPath:C:\Users\max20\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\max20\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\max20\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\max20\.minikube}
I0518 11:29:24.311499   23392 ubuntu.go:177] setting up certificates
I0518 11:29:24.311499   23392 provision.go:84] configureAuth start
I0518 11:29:24.316236   23392 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0518 11:29:24.434818   23392 provision.go:143] copyHostCerts
I0518 11:29:24.443253   23392 exec_runner.go:144] found C:\Users\max20\.minikube/cert.pem, removing ...
I0518 11:29:24.443253   23392 exec_runner.go:203] rm: C:\Users\max20\.minikube\cert.pem
I0518 11:29:24.443757   23392 exec_runner.go:151] cp: C:\Users\max20\.minikube\certs\cert.pem --> C:\Users\max20\.minikube/cert.pem (1119 bytes)
I0518 11:29:24.452148   23392 exec_runner.go:144] found C:\Users\max20\.minikube/key.pem, removing ...
I0518 11:29:24.452148   23392 exec_runner.go:203] rm: C:\Users\max20\.minikube\key.pem
I0518 11:29:24.452148   23392 exec_runner.go:151] cp: C:\Users\max20\.minikube\certs\key.pem --> C:\Users\max20\.minikube/key.pem (1675 bytes)
I0518 11:29:24.460068   23392 exec_runner.go:144] found C:\Users\max20\.minikube/ca.pem, removing ...
I0518 11:29:24.460068   23392 exec_runner.go:203] rm: C:\Users\max20\.minikube\ca.pem
I0518 11:29:24.460068   23392 exec_runner.go:151] cp: C:\Users\max20\.minikube\certs\ca.pem --> C:\Users\max20\.minikube/ca.pem (1074 bytes)
I0518 11:29:24.461241   23392 provision.go:117] generating server cert: C:\Users\max20\.minikube\machines\server.pem ca-key=C:\Users\max20\.minikube\certs\ca.pem private-key=C:\Users\max20\.minikube\certs\ca-key.pem org=max20.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0518 11:29:24.571060   23392 provision.go:177] copyRemoteCerts
I0518 11:29:24.584506   23392 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0518 11:29:24.588349   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:24.724247   23392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63147 SSHKeyPath:C:\Users\max20\.minikube\machines\minikube\id_rsa Username:docker}
I0518 11:29:24.826762   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0518 11:29:24.846281   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0518 11:29:24.864415   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0518 11:29:24.882746   23392 provision.go:87] duration metric: took 571.2462ms to configureAuth
I0518 11:29:24.882746   23392 ubuntu.go:193] setting minikube options for container-runtime
I0518 11:29:24.883270   23392 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0518 11:29:24.886399   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:24.994061   23392 main.go:141] libmachine: Using SSH client type: native
I0518 11:29:24.994575   23392 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd3a1c0] 0xd3cda0 <nil>  [] 0s} 127.0.0.1 63147 <nil> <nil>}
I0518 11:29:24.994575   23392 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0518 11:29:25.125646   23392 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0518 11:29:25.125646   23392 ubuntu.go:71] root file system type: overlay
I0518 11:29:25.125646   23392 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0518 11:29:25.128864   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:25.242265   23392 main.go:141] libmachine: Using SSH client type: native
I0518 11:29:25.242808   23392 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd3a1c0] 0xd3cda0 <nil>  [] 0s} 127.0.0.1 63147 <nil> <nil>}
I0518 11:29:25.242808   23392 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0518 11:29:25.381782   23392 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0518 11:29:25.385724   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:25.506431   23392 main.go:141] libmachine: Using SSH client type: native
I0518 11:29:25.506431   23392 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd3a1c0] 0xd3cda0 <nil>  [] 0s} 127.0.0.1 63147 <nil> <nil>}
I0518 11:29:25.506431   23392 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0518 11:29:25.648677   23392 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0518 11:29:25.648677   23392 machine.go:97] duration metric: took 2.2846583s to provisionDockerMachine
I0518 11:29:25.648677   23392 start.go:293] postStartSetup for "minikube" (driver="docker")
I0518 11:29:25.648677   23392 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0518 11:29:25.653990   23392 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0518 11:29:25.657124   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:25.774454   23392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63147 SSHKeyPath:C:\Users\max20\.minikube\machines\minikube\id_rsa Username:docker}
I0518 11:29:25.884993   23392 ssh_runner.go:195] Run: cat /etc/os-release
I0518 11:29:25.888726   23392 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0518 11:29:25.888726   23392 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0518 11:29:25.888726   23392 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0518 11:29:25.888726   23392 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0518 11:29:25.889249   23392 filesync.go:126] Scanning C:\Users\max20\.minikube\addons for local assets ...
I0518 11:29:25.889249   23392 filesync.go:126] Scanning C:\Users\max20\.minikube\files for local assets ...
I0518 11:29:25.889249   23392 start.go:296] duration metric: took 240.5718ms for postStartSetup
I0518 11:29:25.894400   23392 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0518 11:29:25.897517   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:26.010795   23392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63147 SSHKeyPath:C:\Users\max20\.minikube\machines\minikube\id_rsa Username:docker}
I0518 11:29:26.107571   23392 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0518 11:29:26.112483   23392 fix.go:56] duration metric: took 3.8674216s for fixHost
I0518 11:29:26.113537   23392 start.go:83] releasing machines lock for "minikube", held for 3.8684754s
I0518 11:29:26.116618   23392 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0518 11:29:26.222430   23392 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0518 11:29:26.227202   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:26.228307   23392 ssh_runner.go:195] Run: cat /version.json
I0518 11:29:26.231655   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:26.355403   23392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63147 SSHKeyPath:C:\Users\max20\.minikube\machines\minikube\id_rsa Username:docker}
I0518 11:29:26.370768   23392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63147 SSHKeyPath:C:\Users\max20\.minikube\machines\minikube\id_rsa Username:docker}
I0518 11:29:26.448142   23392 ssh_runner.go:195] Run: systemctl --version
I0518 11:29:27.069013   23392 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0518 11:29:27.077923   23392 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0518 11:29:27.086850   23392 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0518 11:29:27.092087   23392 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0518 11:29:27.099079   23392 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0518 11:29:27.099079   23392 start.go:494] detecting cgroup driver to use...
I0518 11:29:27.099079   23392 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0518 11:29:27.099585   23392 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0518 11:29:27.115595   23392 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0518 11:29:27.132318   23392 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0518 11:29:27.141659   23392 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0518 11:29:27.146821   23392 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0518 11:29:27.159598   23392 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0518 11:29:27.172071   23392 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0518 11:29:27.184599   23392 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0518 11:29:27.197456   23392 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0518 11:29:27.209435   23392 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0518 11:29:27.221466   23392 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0518 11:29:27.234519   23392 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0518 11:29:27.247330   23392 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0518 11:29:27.259694   23392 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0518 11:29:27.271343   23392 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 11:29:27.356616   23392 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0518 11:29:27.464049   23392 start.go:494] detecting cgroup driver to use...
I0518 11:29:27.464049   23392 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0518 11:29:27.470026   23392 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0518 11:29:27.480659   23392 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0518 11:29:27.486167   23392 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0518 11:29:27.498160   23392 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0518 11:29:27.517144   23392 ssh_runner.go:195] Run: which cri-dockerd
I0518 11:29:27.527280   23392 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0518 11:29:27.535888   23392 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0518 11:29:27.584447   23392 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0518 11:29:27.705958   23392 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0518 11:29:27.802783   23392 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0518 11:29:27.802783   23392 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0518 11:29:27.826853   23392 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 11:29:27.936087   23392 ssh_runner.go:195] Run: sudo systemctl restart docker
I0518 11:29:28.273649   23392 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0518 11:29:28.287727   23392 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0518 11:29:28.301972   23392 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0518 11:29:28.315597   23392 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0518 11:29:28.407294   23392 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0518 11:29:28.507546   23392 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 11:29:28.615058   23392 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0518 11:29:28.631056   23392 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0518 11:29:28.644498   23392 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 11:29:28.744954   23392 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0518 11:29:28.916486   23392 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0518 11:29:28.922907   23392 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0518 11:29:28.926561   23392 start.go:562] Will wait 60s for crictl version
I0518 11:29:28.931749   23392 ssh_runner.go:195] Run: which crictl
I0518 11:29:28.941942   23392 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0518 11:29:29.028033   23392 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0518 11:29:29.031144   23392 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0518 11:29:29.095529   23392 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0518 11:29:29.115806   23392 out.go:204] üê≥  –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è Kubernetes v1.30.0 –Ω–∞ Docker 26.0.1 ...
I0518 11:29:29.119455   23392 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0518 11:29:29.294145   23392 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0518 11:29:29.299738   23392 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0518 11:29:29.303415   23392 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0518 11:29:29.315388   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0518 11:29:29.423438   23392 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\max20:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0518 11:29:29.423946   23392 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0518 11:29:29.426546   23392 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0518 11:29:29.443381   23392 docker.go:685] Got preloaded images: -- stdout --
kubernetesui/dashboard-api:1.6.0
kong:3.6
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
kubernetesui/dashboard-web:1.3.0
kubernetesui/dashboard-auth:1.1.3
registry.k8s.io/metrics-server/metrics-server:<none>
kubernetesui/dashboard-metrics-scraper:1.1.1
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0518 11:29:29.443381   23392 docker.go:615] Images already preloaded, skipping extraction
I0518 11:29:29.446475   23392 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0518 11:29:29.462558   23392 docker.go:685] Got preloaded images: -- stdout --
kubernetesui/dashboard-api:1.6.0
kong:3.6
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
kubernetesui/dashboard-web:1.3.0
kubernetesui/dashboard-auth:1.1.3
registry.k8s.io/metrics-server/metrics-server:<none>
kubernetesui/dashboard-metrics-scraper:1.1.1
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0518 11:29:29.462558   23392 cache_images.go:84] Images are preloaded, skipping loading
I0518 11:29:29.462558   23392 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0518 11:29:29.463080   23392 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0518 11:29:29.466156   23392 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0518 11:29:29.596698   23392 cni.go:84] Creating CNI manager for ""
I0518 11:29:29.596698   23392 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0518 11:29:29.597223   23392 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0518 11:29:29.597223   23392 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0518 11:29:29.597223   23392 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0518 11:29:29.601843   23392 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0518 11:29:29.611047   23392 binaries.go:44] Found k8s binaries, skipping transfer
I0518 11:29:29.616233   23392 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0518 11:29:29.623150   23392 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0518 11:29:29.636765   23392 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0518 11:29:29.649293   23392 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0518 11:29:29.668238   23392 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0518 11:29:29.671869   23392 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0518 11:29:29.685269   23392 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 11:29:29.779905   23392 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0518 11:29:29.794482   23392 certs.go:68] Setting up C:\Users\max20\.minikube\profiles\minikube for IP: 192.168.49.2
I0518 11:29:29.794482   23392 certs.go:194] generating shared ca certs ...
I0518 11:29:29.795025   23392 certs.go:226] acquiring lock for ca certs: {Name:mk801b678be9386720be019cb926fe2ef97ad63a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 11:29:29.802397   23392 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\max20\.minikube\ca.key
I0518 11:29:29.816015   23392 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\max20\.minikube\proxy-client-ca.key
I0518 11:29:29.816015   23392 certs.go:256] generating profile certs ...
I0518 11:29:29.816532   23392 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\max20\.minikube\profiles\minikube\client.key
I0518 11:29:29.830412   23392 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\max20\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0518 11:29:29.844993   23392 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\max20\.minikube\profiles\minikube\proxy-client.key
I0518 11:29:29.847849   23392 certs.go:484] found cert: C:\Users\max20\.minikube\certs\ca-key.pem (1675 bytes)
I0518 11:29:29.847849   23392 certs.go:484] found cert: C:\Users\max20\.minikube\certs\ca.pem (1074 bytes)
I0518 11:29:29.848355   23392 certs.go:484] found cert: C:\Users\max20\.minikube\certs\cert.pem (1119 bytes)
I0518 11:29:29.848355   23392 certs.go:484] found cert: C:\Users\max20\.minikube\certs\key.pem (1675 bytes)
I0518 11:29:29.849674   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0518 11:29:29.876684   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0518 11:29:29.901617   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0518 11:29:29.931863   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0518 11:29:29.956174   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0518 11:29:29.981443   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0518 11:29:30.004825   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0518 11:29:30.033070   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0518 11:29:30.057355   23392 ssh_runner.go:362] scp C:\Users\max20\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0518 11:29:30.097554   23392 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0518 11:29:30.193304   23392 ssh_runner.go:195] Run: openssl version
I0518 11:29:30.216717   23392 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0518 11:29:30.290695   23392 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0518 11:29:30.297689   23392 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 16 07:27 /usr/share/ca-certificates/minikubeCA.pem
I0518 11:29:30.306884   23392 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0518 11:29:30.329580   23392 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0518 11:29:30.387253   23392 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0518 11:29:30.400094   23392 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0518 11:29:30.419286   23392 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0518 11:29:30.439609   23392 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0518 11:29:30.502201   23392 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0518 11:29:30.516889   23392 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0518 11:29:30.535318   23392 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0518 11:29:30.546380   23392 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\max20:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0518 11:29:30.549575   23392 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0518 11:29:30.612651   23392 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0518 11:29:30.627325   23392 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0518 11:29:30.627325   23392 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0518 11:29:30.627325   23392 kubeadm.go:587] restartPrimaryControlPlane start ...
I0518 11:29:30.632497   23392 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0518 11:29:30.686757   23392 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0518 11:29:30.696883   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0518 11:29:30.863252   23392 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:64022"
I0518 11:29:30.863252   23392 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:64022, want: 127.0.0.1:63151
I0518 11:29:30.863762   23392 kubeconfig.go:62] C:\Users\max20\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I0518 11:29:30.864787   23392 lock.go:35] WriteFile acquiring C:\Users\max20\.kube\config: {Name:mk98bf36ca6c9430c59b98fad5ba6410defc8f6e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 11:29:30.891885   23392 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0518 11:29:30.907349   23392 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0518 11:29:30.907349   23392 kubeadm.go:591] duration metric: took 280.0234ms to restartPrimaryControlPlane
I0518 11:29:30.907349   23392 kubeadm.go:393] duration metric: took 360.9692ms to StartCluster
I0518 11:29:30.907349   23392 settings.go:142] acquiring lock: {Name:mk426478a3bcda4d5ef69aba2960e8e0007b6cc9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 11:29:30.907349   23392 settings.go:150] Updating kubeconfig:  C:\Users\max20\.kube\config
I0518 11:29:30.908419   23392 lock.go:35] WriteFile acquiring C:\Users\max20\.kube\config: {Name:mk98bf36ca6c9430c59b98fad5ba6410defc8f6e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 11:29:30.909483   23392 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0518 11:29:30.910024   23392 out.go:177] üîé  –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Kubernetes –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è ...
I0518 11:29:30.909483   23392 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0518 11:29:30.909483   23392 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0518 11:29:30.910546   23392 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0518 11:29:30.910546   23392 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0518 11:29:30.910546   23392 addons.go:243] addon storage-provisioner should already be in state true
I0518 11:29:30.910546   23392 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0518 11:29:30.910546   23392 addons.go:69] Setting metrics-server=true in profile "minikube"
I0518 11:29:30.910546   23392 addons.go:69] Setting dashboard=true in profile "minikube"
I0518 11:29:30.910546   23392 addons.go:234] Setting addon metrics-server=true in "minikube"
I0518 11:29:30.910546   23392 addons.go:234] Setting addon dashboard=true in "minikube"
W0518 11:29:30.910546   23392 addons.go:243] addon dashboard should already be in state true
W0518 11:29:30.910546   23392 addons.go:243] addon metrics-server should already be in state true
I0518 11:29:30.910546   23392 host.go:66] Checking if "minikube" exists ...
I0518 11:29:30.911193   23392 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0518 11:29:30.911193   23392 host.go:66] Checking if "minikube" exists ...
I0518 11:29:30.911193   23392 host.go:66] Checking if "minikube" exists ...
I0518 11:29:30.921509   23392 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 11:29:30.926454   23392 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 11:29:30.931802   23392 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 11:29:30.937788   23392 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 11:29:30.939299   23392 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 11:29:31.140111   23392 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ registry.k8s.io/metrics-server/metrics-server:v0.7.1
I0518 11:29:31.140805   23392 addons.go:426] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0518 11:29:31.140805   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0518 11:29:31.143908   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:31.157466   23392 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ gcr.io/k8s-minikube/storage-provisioner:v5
I0518 11:29:31.157022   23392 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ docker.io/kubernetesui/dashboard:v2.7.0
I0518 11:29:31.160056   23392 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0518 11:29:31.160742   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0518 11:29:31.162031   23392 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ docker.io/kubernetesui/metrics-scraper:v1.0.8
I0518 11:29:31.163069   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0518 11:29:31.163069   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0518 11:29:31.168884   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:31.169423   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:31.188770   23392 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0518 11:29:31.188770   23392 addons.go:243] addon default-storageclass should already be in state true
I0518 11:29:31.188770   23392 host.go:66] Checking if "minikube" exists ...
I0518 11:29:31.219872   23392 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 11:29:31.398941   23392 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0518 11:29:31.405459   23392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63147 SSHKeyPath:C:\Users\max20\.minikube\machines\minikube\id_rsa Username:docker}
I0518 11:29:31.437831   23392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63147 SSHKeyPath:C:\Users\max20\.minikube\machines\minikube\id_rsa Username:docker}
I0518 11:29:31.453848   23392 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0518 11:29:31.453848   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0518 11:29:31.453848   23392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63147 SSHKeyPath:C:\Users\max20\.minikube\machines\minikube\id_rsa Username:docker}
I0518 11:29:31.458395   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 11:29:31.492756   23392 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0518 11:29:31.655109   23392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63147 SSHKeyPath:C:\Users\max20\.minikube\machines\minikube\id_rsa Username:docker}
I0518 11:29:31.671174   23392 api_server.go:52] waiting for apiserver process to appear ...
I0518 11:29:31.682869   23392 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 11:29:31.780252   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0518 11:29:31.780252   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0518 11:29:31.781267   23392 addons.go:426] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0518 11:29:31.781267   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0518 11:29:31.814147   23392 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0518 11:29:31.888324   23392 addons.go:426] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0518 11:29:31.888324   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0518 11:29:31.897908   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0518 11:29:31.897908   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0518 11:29:31.980678   23392 addons.go:426] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0518 11:29:31.980678   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0518 11:29:31.996867   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0518 11:29:31.996867   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0518 11:29:31.999707   23392 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0518 11:29:32.020836   23392 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0518 11:29:32.096507   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0518 11:29:32.096507   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0518 11:29:32.187871   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-role.yaml
I0518 11:29:32.187871   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0518 11:29:32.196344   23392 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 11:29:32.278265   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0518 11:29:32.278265   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0518 11:29:32.309635   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0518 11:29:32.309635   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0518 11:29:32.389692   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0518 11:29:32.389888   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0518 11:29:32.481626   23392 addons.go:426] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0518 11:29:32.481626   23392 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
W0518 11:29:32.583334   23392 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0518 11:29:32.583334   23392 retry.go:31] will retry after 302.17817ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0518 11:29:32.583334   23392 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0518 11:29:32.583334   23392 retry.go:31] will retry after 337.266442ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0518 11:29:32.583902   23392 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0518 11:29:32.583902   23392 retry.go:31] will retry after 234.557421ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0518 11:29:32.583902   23392 api_server.go:72] duration metric: took 1.6744186s to wait for apiserver process to appear ...
I0518 11:29:32.583902   23392 api_server.go:88] waiting for apiserver healthz status ...
I0518 11:29:32.583902   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:32.586438   23392 api_server.go:269] stopped: https://127.0.0.1:63151/healthz: Get "https://127.0.0.1:63151/healthz": EOF
I0518 11:29:32.595681   23392 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0518 11:29:32.727294   23392 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0518 11:29:32.727294   23392 retry.go:31] will retry after 144.324207ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0518 11:29:32.826893   23392 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0518 11:29:32.880543   23392 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0518 11:29:32.896862   23392 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0518 11:29:32.939523   23392 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0518 11:29:33.088889   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:34.908587   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0518 11:29:34.908587   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0518 11:29:34.908587   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:34.992546   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0518 11:29:34.993083   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0518 11:29:35.087041   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:35.199176   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:35.199176   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:35.587650   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:35.598095   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:35.598612   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:36.087176   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:36.097427   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:36.097427   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:36.587020   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:36.601358   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:36.601358   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:37.088840   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:37.108068   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:37.108068   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:37.588344   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:37.597034   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:37.597034   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:38.087424   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:38.096019   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:38.096019   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:38.588757   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:38.690284   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:38.690795   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:38.803111   23392 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (5.976218s)
I0518 11:29:38.803111   23392 addons.go:470] Verifying addon metrics-server=true in "minikube"
I0518 11:29:39.086312   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:39.107850   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:39.108411   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:39.587456   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:39.599645   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0518 11:29:39.599645   23392 api_server.go:103] status: https://127.0.0.1:63151/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0518 11:29:40.086824   23392 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63151/healthz ...
I0518 11:29:40.109509   23392 api_server.go:279] https://127.0.0.1:63151/healthz returned 200:
ok
I0518 11:29:40.191064   23392 api_server.go:141] control plane version: v1.30.0
I0518 11:29:40.191064   23392 api_server.go:131] duration metric: took 7.6071624s to wait for apiserver health ...
I0518 11:29:40.191064   23392 system_pods.go:43] waiting for kube-system pods to appear ...
I0518 11:29:40.208915   23392 system_pods.go:59] 8 kube-system pods found
I0518 11:29:40.208915   23392 system_pods.go:61] "coredns-7db6d8ff4d-62hfb" [74ea07bb-e427-4645-a65f-ab5ffd94f2e7] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0518 11:29:40.208915   23392 system_pods.go:61] "etcd-minikube" [fa8cd569-9995-4fdf-b1b2-6a1bcf248e60] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0518 11:29:40.209423   23392 system_pods.go:61] "kube-apiserver-minikube" [c4b4ae44-81d6-49ba-ba1a-a646700b44b8] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0518 11:29:40.209423   23392 system_pods.go:61] "kube-controller-manager-minikube" [d3568e1e-9707-4d85-8358-dca3dfd70c77] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0518 11:29:40.209423   23392 system_pods.go:61] "kube-proxy-jswzc" [ca7ec293-cf6f-4c95-9ff2-a9400c23e478] Running
I0518 11:29:40.209423   23392 system_pods.go:61] "kube-scheduler-minikube" [8b437409-c225-4dc1-addf-ee3944a0f374] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0518 11:29:40.209423   23392 system_pods.go:61] "metrics-server-c59844bb4-9r8k4" [b0f8b5cc-36e1-42a7-9919-e2c9274300cf] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0518 11:29:40.209423   23392 system_pods.go:61] "storage-provisioner" [8adc98ec-3e00-4b8d-8657-eb760621d3e9] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0518 11:29:40.209423   23392 system_pods.go:74] duration metric: took 18.3586ms to wait for pod list to return data ...
I0518 11:29:40.209423   23392 kubeadm.go:576] duration metric: took 9.2999396s to wait for: map[apiserver:true system_pods:true]
I0518 11:29:40.209423   23392 node_conditions.go:102] verifying NodePressure condition ...
I0518 11:29:40.293942   23392 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0518 11:29:40.293942   23392 node_conditions.go:123] node cpu capacity is 8
I0518 11:29:40.293942   23392 node_conditions.go:105] duration metric: took 84.5198ms to run NodePressure ...
I0518 11:29:40.293942   23392 start.go:240] waiting for startup goroutines ...
I0518 11:29:41.525582   23392 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (8.6450388s)
I0518 11:29:41.525582   23392 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (8.5860592s)
I0518 11:29:41.525582   23392 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (8.6287199s)
I0518 11:29:41.526127   23392 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0518 11:29:41.582166   23392 out.go:177] üåü  –í–∫–ª—é—á–µ–Ω–Ω—ã–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è: metrics-server, storage-provisioner, dashboard, default-storageclass
I0518 11:29:41.584554   23392 addons.go:505] duration metric: took 10.6750711s for enable addons: enabled=[metrics-server storage-provisioner dashboard default-storageclass]
I0518 11:29:41.585062   23392 start.go:245] waiting for cluster config update ...
I0518 11:29:41.585062   23392 start.go:254] writing updated cluster config ...
I0518 11:29:41.599304   23392 ssh_runner.go:195] Run: rm -f paused
I0518 11:29:41.725634   23392 start.go:600] kubectl: 1.29.1, cluster: 1.30.0 (minor skew: 1)
I0518 11:29:41.726715   23392 out.go:177] üèÑ  –ì–æ—Ç–æ–≤–æ! kubectl –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ "minikube" –∏ "default" –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–º—ë–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é


==> Docker <==
May 18 08:29:38 minikube cri-dockerd[1320]: time="2024-05-18T08:29:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/308f5a991b00d3af367175e183956ddcfb67621b29866b7c6ee7621ebbafb78b/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboar.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 08:29:38 minikube cri-dockerd[1320]: time="2024-05-18T08:29:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/751017ec79d6497ef4568ab57361772cfa7005b7b1aa8c4849a9c8c4e0506929/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboar.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 08:29:38 minikube cri-dockerd[1320]: time="2024-05-18T08:29:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7ba63be27930a0316306b88026ba98b63fadd08ea3159122ee7ba6edd19347ae/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 08:29:39 minikube cri-dockerd[1320]: time="2024-05-18T08:29:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9b01b5152f812c21fb074f2d709e596b49f2c52a04dc2f32975af379539097be/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboar.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 08:29:39 minikube cri-dockerd[1320]: time="2024-05-18T08:29:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0d46f4e77a14126e348301f20cfbf2a8888314a0d91a920f898fcfe9d44acde1/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 08:29:39 minikube dockerd[1051]: time="2024-05-18T08:29:39.787553203Z" level=info msg="ignoring event" container=8ce0ce4d04831bacbe7caa6bf3b1ab95f7adde5f8d9acd60dd712b4caf3711c3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 08:29:49 minikube dockerd[1051]: time="2024-05-18T08:29:49.622345695Z" level=info msg="ignoring event" container=06feb258ea146dff4fcf2c96af57b7a284f58387afc33cd7ced2d85edd365fe3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 08:29:58 minikube cri-dockerd[1320]: time="2024-05-18T08:29:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4450b905dafc27f71c001b3beb29dbbace8f4a4884fa91e3b7c18d9c6323bdc9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 08:30:00 minikube dockerd[1051]: time="2024-05-18T08:30:00.670155247Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=deca6a3f1b00cce8 traceID=d22be5ebce0f0961de52b15ef23f7edc
May 18 08:30:19 minikube dockerd[1051]: time="2024-05-18T08:30:19.406113240Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=6aa75923867a9870 traceID=1a34c270598a70aede2b55bc50f2b385
May 18 08:30:46 minikube dockerd[1051]: time="2024-05-18T08:30:46.355243329Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=b1abee5371486b97 traceID=fdf518bccb1bf75ede2d1fadd4746da2
May 18 08:30:52 minikube dockerd[1051]: time="2024-05-18T08:30:52.306453986Z" level=info msg="ignoring event" container=4450b905dafc27f71c001b3beb29dbbace8f4a4884fa91e3b7c18d9c6323bdc9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 08:31:54 minikube cri-dockerd[1320]: time="2024-05-18T08:31:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c7f62c9075556f625afcddc077c565ad1fc8d29ba3502ba99fe257936b783aec/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 08:32:05 minikube cri-dockerd[1320]: time="2024-05-18T08:32:05Z" level=info msg="Stop pulling image maksymposkannyi/kub-first-app:tagname: Status: Downloaded newer image for maksymposkannyi/kub-first-app:tagname"
May 18 11:00:35 minikube dockerd[1051]: time="2024-05-18T11:00:35.364477161Z" level=info msg="ignoring event" container=4dc226e3db2e776c413d77b64d4754ac931e5140e67fc141cd7dee95e8c30e4b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 11:00:41 minikube dockerd[1051]: time="2024-05-18T11:00:41.525080388Z" level=info msg="ignoring event" container=43bfe2ca44c160dcce49dc5817c5220abcbf6c1e11541a8b6e68dff988d57e2f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 11:27:04 minikube cri-dockerd[1320]: time="2024-05-18T11:27:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/85534ed480d937b03a4a7be7bc4abc0164d30a61726fd0f599dbe1d5a4c70057/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 11:27:04 minikube cri-dockerd[1320]: time="2024-05-18T11:27:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e01a4b59dcceb99f187620a543456e887d08712329574f410b8a5b730fa816e5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 11:39:19 minikube cri-dockerd[1320]: time="2024-05-18T11:39:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/16e23a496fe1cb00e0af7b956eb5991e3f9788d09be6f810d2d742a1a9a70fb2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 11:39:22 minikube cri-dockerd[1320]: time="2024-05-18T11:39:22Z" level=info msg="Stop pulling image maksymposkannyi/kub-first-app:2: Status: Downloaded newer image for maksymposkannyi/kub-first-app:2"
May 18 11:39:24 minikube cri-dockerd[1320]: time="2024-05-18T11:39:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6d9f4490ee9d9b239d55da883bc50a20ce696950b85b091194a9b4f9d59e1963/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 11:39:27 minikube cri-dockerd[1320]: time="2024-05-18T11:39:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/16bcff1f6250ef849736ca5d4bcbac020140246aeddaad337eafbc050228dd77/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 11:39:54 minikube dockerd[1051]: time="2024-05-18T11:39:54.124577560Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=90bf8628f89750c46f179dfbec601aef30be7365b71b1ea4c15a9635a6dda928 spanID=1fd8e365308f44ad traceID=fda06fc867c07a2af38a3ec897d8da65
May 18 11:39:54 minikube dockerd[1051]: time="2024-05-18T11:39:54.157007624Z" level=info msg="ignoring event" container=90bf8628f89750c46f179dfbec601aef30be7365b71b1ea4c15a9635a6dda928 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 11:39:54 minikube dockerd[1051]: time="2024-05-18T11:39:54.402805579Z" level=info msg="ignoring event" container=85534ed480d937b03a4a7be7bc4abc0164d30a61726fd0f599dbe1d5a4c70057 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 11:39:56 minikube dockerd[1051]: time="2024-05-18T11:39:56.116609391Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=1a16f72910773411039a788f99b652b505bce96f8e7a40359e5abda3cc9b23f9 spanID=9064bb4d5b126b2b traceID=be5d8e6f0f9b917dee5b85cf8283819e
May 18 11:39:56 minikube dockerd[1051]: time="2024-05-18T11:39:56.135315401Z" level=info msg="ignoring event" container=1a16f72910773411039a788f99b652b505bce96f8e7a40359e5abda3cc9b23f9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 11:39:56 minikube dockerd[1051]: time="2024-05-18T11:39:56.328735569Z" level=info msg="ignoring event" container=e01a4b59dcceb99f187620a543456e887d08712329574f410b8a5b730fa816e5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 11:39:58 minikube dockerd[1051]: time="2024-05-18T11:39:58.329691400Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=6dd2aa46786455508198a1aa61d04428a6833081ff09c524a3f335e336b8f663 spanID=5a9a9b3413ddd694 traceID=1e772a398811cdd7c503053c10638a2a
May 18 11:39:58 minikube dockerd[1051]: time="2024-05-18T11:39:58.351654246Z" level=info msg="ignoring event" container=6dd2aa46786455508198a1aa61d04428a6833081ff09c524a3f335e336b8f663 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 11:39:58 minikube dockerd[1051]: time="2024-05-18T11:39:58.534954901Z" level=info msg="ignoring event" container=c7f62c9075556f625afcddc077c565ad1fc8d29ba3502ba99fe257936b783aec module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 11:40:01 minikube cri-dockerd[1320]: time="2024-05-18T11:40:01Z" level=error msg="error getting RW layer size for container ID '6dd2aa46786455508198a1aa61d04428a6833081ff09c524a3f335e336b8f663': Error response from daemon: No such container: 6dd2aa46786455508198a1aa61d04428a6833081ff09c524a3f335e336b8f663"
May 18 11:40:01 minikube cri-dockerd[1320]: time="2024-05-18T11:40:01Z" level=error msg="Set backoffDuration to : 1m0s for container ID '6dd2aa46786455508198a1aa61d04428a6833081ff09c524a3f335e336b8f663'"
May 18 12:10:29 minikube dockerd[1051]: time="2024-05-18T12:10:29.637215855Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=c04791c0ff9a7427b1dcede6283e00193642a4c1a2cd45732aca2192a561ec4d spanID=b3a0220869b393aa traceID=73552b95f5dcbca381e3c8f6121740f9
May 18 12:10:29 minikube dockerd[1051]: time="2024-05-18T12:10:29.638353054Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=8dddabdbecf9ee7c7f896aad8f3154d5bc05152dd45f8198a748238e76ebb049 spanID=e5e9dff8aec0566b traceID=f7fc605d4e75c8c53d4c9aa684d09d9b
May 18 12:10:29 minikube dockerd[1051]: time="2024-05-18T12:10:29.640545553Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=483fb07acbcf4a813c7f9f3edb5b9eed0ec04fc6aa35db09fa490bd5fdc7cf53 spanID=3758aafa7c1c5e01 traceID=26a5f2382c26aadecebfba693cf26e35
May 18 12:10:29 minikube dockerd[1051]: time="2024-05-18T12:10:29.677431134Z" level=info msg="ignoring event" container=8dddabdbecf9ee7c7f896aad8f3154d5bc05152dd45f8198a748238e76ebb049 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 12:10:29 minikube dockerd[1051]: time="2024-05-18T12:10:29.678634633Z" level=info msg="ignoring event" container=483fb07acbcf4a813c7f9f3edb5b9eed0ec04fc6aa35db09fa490bd5fdc7cf53 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 12:10:29 minikube dockerd[1051]: time="2024-05-18T12:10:29.679923332Z" level=info msg="ignoring event" container=c04791c0ff9a7427b1dcede6283e00193642a4c1a2cd45732aca2192a561ec4d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 12:10:29 minikube dockerd[1051]: time="2024-05-18T12:10:29.940051495Z" level=info msg="ignoring event" container=16e23a496fe1cb00e0af7b956eb5991e3f9788d09be6f810d2d742a1a9a70fb2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 12:10:30 minikube dockerd[1051]: time="2024-05-18T12:10:30.063468930Z" level=info msg="ignoring event" container=16bcff1f6250ef849736ca5d4bcbac020140246aeddaad337eafbc050228dd77 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 12:10:30 minikube dockerd[1051]: time="2024-05-18T12:10:30.145920786Z" level=info msg="ignoring event" container=6d9f4490ee9d9b239d55da883bc50a20ce696950b85b091194a9b4f9d59e1963 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 12:46:42 minikube cri-dockerd[1320]: time="2024-05-18T12:46:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7e3a9aa4729eb1a23b7ff3a4b3131d4d879cfe2d098c78002e2c5b7278b93a64/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 13:11:16 minikube dockerd[1051]: time="2024-05-18T13:11:16.218071918Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=eea5b164970efd0a16d4c0169e1f1a30566ed8b2c4845287b2770ca6e3f2c1f9 spanID=0272df96791341dc traceID=d3ec05f617b02ccfc84ec391306a1cdf
May 18 13:11:16 minikube dockerd[1051]: time="2024-05-18T13:11:16.241682886Z" level=info msg="ignoring event" container=eea5b164970efd0a16d4c0169e1f1a30566ed8b2c4845287b2770ca6e3f2c1f9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 13:11:16 minikube cri-dockerd[1320]: time="2024-05-18T13:11:16Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"second-app-deployment-64b97cd45c-fcrh9_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
May 18 13:11:16 minikube dockerd[1051]: time="2024-05-18T13:11:16.506999312Z" level=info msg="ignoring event" container=7e3a9aa4729eb1a23b7ff3a4b3131d4d879cfe2d098c78002e2c5b7278b93a64 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 13:11:42 minikube cri-dockerd[1320]: time="2024-05-18T13:11:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e4271e010e86f60ce5afc624cbf02bc79338f51bd3ffa5676f41010c2f7cdd2c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 13:13:38 minikube dockerd[1051]: time="2024-05-18T13:13:38.255803827Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=9696d02d7d11422dd686f7f456e0dc053f8107b1068e3e4fdf58bb716daefb88 spanID=a9691053c1537cfc traceID=22c8b27abec7a7a65b33c66f0383dddb
May 18 13:13:38 minikube dockerd[1051]: time="2024-05-18T13:13:38.277085452Z" level=info msg="ignoring event" container=9696d02d7d11422dd686f7f456e0dc053f8107b1068e3e4fdf58bb716daefb88 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 13:13:38 minikube dockerd[1051]: time="2024-05-18T13:13:38.503943350Z" level=info msg="ignoring event" container=e4271e010e86f60ce5afc624cbf02bc79338f51bd3ffa5676f41010c2f7cdd2c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 13:14:32 minikube cri-dockerd[1320]: time="2024-05-18T13:14:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/93a6c0df202368d5513397e6d71ef9b370f804852d71a6798d9d815ca0516c88/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 13:14:35 minikube dockerd[1051]: time="2024-05-18T13:14:35.109996890Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=947fe0ae5d5049f9 traceID=6a4c9865e743102b2e0b6d9cb3bcca6c
May 18 13:14:35 minikube dockerd[1051]: time="2024-05-18T13:14:35.110073493Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 13:14:51 minikube dockerd[1051]: time="2024-05-18T13:14:51.596178306Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=244044a78e714db5 traceID=a8e84b9b5999ee4baf82815f139ac020
May 18 13:14:51 minikube dockerd[1051]: time="2024-05-18T13:14:51.596250005Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 13:15:17 minikube dockerd[1051]: time="2024-05-18T13:15:17.502492644Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=d089e6731b4571af traceID=110252bf5240d8ed2530bee8d4426f60
May 18 13:15:17 minikube dockerd[1051]: time="2024-05-18T13:15:17.502578448Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 18 13:16:10 minikube dockerd[1051]: time="2024-05-18T13:16:10.857603974Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=fa1d84c4f7b19204 traceID=9a525150d543d59650143705dfbbd2b6
May 18 13:16:10 minikube dockerd[1051]: time="2024-05-18T13:16:10.857702176Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                                CREATED             STATE               NAME                                   ATTEMPT             POD ID              POD
dbf88e325ac8e       6e38f40d628db                                                                                        5 hours ago         Running             storage-provisioner                    6                   b4bd9c8deaf6e       storage-provisioner
51e80e22cc09d       ea534d4b2a6d5                                                                                        5 hours ago         Running             proxy                                  1                   698217bbe4901       kubernetes-dashboard-kong-7696bb8c88-2jfx5
6863e3930a7ac       8d393eaec12e7                                                                                        5 hours ago         Running             kubernetes-dashboard-web               1                   751017ec79d64       kubernetes-dashboard-web-6557fb6496-8zr5n
6f665fe8c1362       19b0f210f0dce                                                                                        5 hours ago         Running             kubernetes-dashboard-auth              2                   9b01b5152f812       kubernetes-dashboard-auth-66cf496dfc-4kjgx
0a207c8e379a3       f645cc253df05                                                                                        5 hours ago         Running             kubernetes-dashboard-api               2                   308f5a991b00d       kubernetes-dashboard-api-6b547b9c5d-r94hb
30cc45cb339d3       115053965e86b                                                                                        5 hours ago         Running             dashboard-metrics-scraper              2                   7ba63be27930a       dashboard-metrics-scraper-b5fc48f67-8dl49
578fc8f47bcf1       a24c7c057ec87                                                                                        5 hours ago         Running             metrics-server                         2                   dc0fd3afb211e       metrics-server-c59844bb4-9r8k4
f2d0c41cc6465       07655ddf2eebe                                                                                        5 hours ago         Running             kubernetes-dashboard                   2                   0d46f4e77a141       kubernetes-dashboard-779776cb65-wwxcr
d7573ac3ddddd       e3e2596959442                                                                                        5 hours ago         Running             kubernetes-dashboard-metrics-scraper   2                   c8b09ada13011       kubernetes-dashboard-metrics-scraper-69cf85488f-v2cg7
8ce0ce4d04831       ea534d4b2a6d5                                                                                        5 hours ago         Exited              clear-stale-pid                        1                   698217bbe4901       kubernetes-dashboard-kong-7696bb8c88-2jfx5
95d415a635418       cbb01a7bd410d                                                                                        5 hours ago         Running             coredns                                2                   322c6a067a9cb       coredns-7db6d8ff4d-62hfb
06feb258ea146       6e38f40d628db                                                                                        5 hours ago         Exited              storage-provisioner                    5                   b4bd9c8deaf6e       storage-provisioner
35ade71b27d9f       a0bf559e280cf                                                                                        5 hours ago         Running             kube-proxy                             2                   ef553cd4f7eb3       kube-proxy-jswzc
8f2dc4a985381       259c8277fcbbc                                                                                        5 hours ago         Running             kube-scheduler                         2                   d43991689b63e       kube-scheduler-minikube
7f7b7e95626f1       c42f13656d0b2                                                                                        5 hours ago         Running             kube-apiserver                         2                   41b490c743c1e       kube-apiserver-minikube
5239868149a77       3861cfcd7c04c                                                                                        5 hours ago         Running             etcd                                   2                   bab090a891ca0       etcd-minikube
66482504396f4       c7aad43836fa5                                                                                        5 hours ago         Running             kube-controller-manager                2                   0abc2feae1ce8       kube-controller-manager-minikube
5edaeb583107c       kubernetesui/dashboard-web@sha256:5dccfb8b8e7fa5456142d011c2960d30f89f47c6e8bfc8ca2678465c58388068   29 hours ago        Exited              kubernetes-dashboard-web               0                   c0c8bc73472b9       kubernetes-dashboard-web-6557fb6496-8zr5n
88c447e0f9883       ea534d4b2a6d5                                                                                        29 hours ago        Exited              proxy                                  0                   1a03962413d64       kubernetes-dashboard-kong-7696bb8c88-2jfx5
933452e1ba3c4       07655ddf2eebe                                                                                        29 hours ago        Exited              kubernetes-dashboard                   1                   e0e3911d0c1bb       kubernetes-dashboard-779776cb65-wwxcr
147d79a6409f6       19b0f210f0dce                                                                                        29 hours ago        Exited              kubernetes-dashboard-auth              1                   77a8d4dadeb23       kubernetes-dashboard-auth-66cf496dfc-4kjgx
6115042b1c605       e3e2596959442                                                                                        29 hours ago        Exited              kubernetes-dashboard-metrics-scraper   1                   97e1448c6e551       kubernetes-dashboard-metrics-scraper-69cf85488f-v2cg7
730ae4a660892       115053965e86b                                                                                        29 hours ago        Exited              dashboard-metrics-scraper              1                   4b177e7753d60       dashboard-metrics-scraper-b5fc48f67-8dl49
fa57b295d235a       f645cc253df05                                                                                        29 hours ago        Exited              kubernetes-dashboard-api               1                   ef27efb425e6f       kubernetes-dashboard-api-6b547b9c5d-r94hb
97d26dd741c68       a24c7c057ec87                                                                                        29 hours ago        Exited              metrics-server                         1                   c6605ca2e1447       metrics-server-c59844bb4-9r8k4
f4f6ea871f362       cbb01a7bd410d                                                                                        29 hours ago        Exited              coredns                                1                   8c6e99fad6924       coredns-7db6d8ff4d-62hfb
c262cca0473a6       a0bf559e280cf                                                                                        29 hours ago        Exited              kube-proxy                             1                   38bf94cf9b698       kube-proxy-jswzc
f071cdf3a3b00       c42f13656d0b2                                                                                        29 hours ago        Exited              kube-apiserver                         1                   6e82562ea0cef       kube-apiserver-minikube
07920dc1e0b74       3861cfcd7c04c                                                                                        29 hours ago        Exited              etcd                                   1                   8f358d3b4fbd4       etcd-minikube
7ed72c6999f1d       c7aad43836fa5                                                                                        29 hours ago        Exited              kube-controller-manager                1                   fb0976f5f9d7a       kube-controller-manager-minikube
e149a9d3084cd       259c8277fcbbc                                                                                        29 hours ago        Exited              kube-scheduler                         1                   3bd4bdd5758ba       kube-scheduler-minikube


==> coredns [95d415a63541] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:55157 - 33022 "HINFO IN 940544701720677438.8952271230739720581. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.0350865s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1281176630]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (18-May-2024 08:29:40.891) (total time: 10007ms):
Trace[1281176630]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10006ms (08:29:50.898)
Trace[1281176630]: [10.007610135s] [10.007610135s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1863691984]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (18-May-2024 08:29:40.891) (total time: 10008ms):
Trace[1863691984]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10007ms (08:29:50.898)
Trace[1863691984]: [10.008336249s] [10.008336249s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[892279825]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (18-May-2024 08:29:40.891) (total time: 10007ms):
Trace[892279825]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10006ms (08:29:50.898)
Trace[892279825]: [10.007674251s] [10.007674251s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] 10.244.0.20:55266 - 52471 "A IN kong-hf.konghq.com.kubernetes-dashboar.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.0008311s
[INFO] 10.244.0.20:56031 - 25399 "A IN kong-hf.konghq.com.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0002311s
[INFO] 10.244.0.20:54486 - 25978 "A IN kong-hf.konghq.com.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0001748s
[INFO] 10.244.0.20:47440 - 27426 "A IN kong-hf.konghq.com. udp 36 false 512" NOERROR qr,rd,ra 104 0.18366773s
[INFO] 10.244.0.20:36784 - 5325 "A IN kong-hf.konghq.com.kubernetes-dashboar.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000323313s
[INFO] 10.244.0.20:57786 - 10222 "A IN kong-hf.konghq.com.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000126805s
[INFO] 10.244.0.20:37366 - 28425 "A IN kong-hf.konghq.com.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000127105s
[INFO] 10.244.0.20:53919 - 44007 "A IN kong-hf.konghq.com. udp 36 false 512" NOERROR qr,rd,ra 104 0.055398305s
[INFO] 10.244.0.20:43552 - 57387 "A IN kong-hf.konghq.com.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000542811s
[INFO] 10.244.0.20:47432 - 24760 "A IN kong-hf.konghq.com.kubernetes-dashboar.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000650713s
[INFO] 10.244.0.20:38441 - 38882 "A IN kong-hf.konghq.com.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000549111s
[INFO] 10.244.0.20:46578 - 8141 "A IN kong-hf.konghq.com. udp 36 false 512" NOERROR qr,rd,ra 104 0.16939323s


==> coredns [f4f6ea871f36] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:57025 - 4437 "HINFO IN 4381436085496771577.8534849216470940701. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.037709953s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[2044339161]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (17-May-2024 07:48:00.431) (total time: 10012ms):
Trace[2044339161]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10009ms (07:48:10.440)
Trace[2044339161]: [10.012430119s] [10.012430119s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[59272759]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (17-May-2024 07:48:00.432) (total time: 10011ms):
Trace[59272759]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10008ms (07:48:10.440)
Trace[59272759]: [10.011533215s] [10.011533215s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[567807486]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (17-May-2024 07:48:00.431) (total time: 10083ms):
Trace[567807486]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10083ms (07:48:10.514)
Trace[567807486]: [10.083601476s] [10.083601476s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_16T10_27_34_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 16 May 2024 07:27:30 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 18 May 2024 13:16:33 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 18 May 2024 13:16:28 +0000   Thu, 16 May 2024 07:27:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 18 May 2024 13:16:28 +0000   Thu, 16 May 2024 07:27:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 18 May 2024 13:16:28 +0000   Thu, 16 May 2024 07:27:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 18 May 2024 13:16:28 +0000   Thu, 16 May 2024 07:27:34 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16265192Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16265192Ki
  pods:               110
System Info:
  Machine ID:                 1b93984c6cf3482d807bbd1f3dff7bea
  System UUID:                1b93984c6cf3482d807bbd1f3dff7bea
  Boot ID:                    e77dda8c-27a1-461e-9df5-ee500488e122
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (16 in total)
  Namespace                   Name                                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                     ------------  ----------  ---------------  -------------  ---
  default                     second-app-deployment-686d5d6ff-54zsm                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m1s
  kube-system                 coredns-7db6d8ff4d-62hfb                                 100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     2d5h
  kube-system                 etcd-minikube                                            100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         2d5h
  kube-system                 kube-apiserver-minikube                                  250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d5h
  kube-system                 kube-controller-manager-minikube                         200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d5h
  kube-system                 kube-proxy-jswzc                                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d5h
  kube-system                 kube-scheduler-minikube                                  100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d5h
  kube-system                 metrics-server-c59844bb4-9r8k4                           100m (1%!)(MISSING)     0 (0%!)(MISSING)      200Mi (1%!)(MISSING)       0 (0%!)(MISSING)         2d5h
  kube-system                 storage-provisioner                                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d5h
  kubernetes-dashboar         kubernetes-dashboard-api-6b547b9c5d-r94hb                100m (1%!)(MISSING)     250m (3%!)(MISSING)   200Mi (1%!)(MISSING)       400Mi (2%!)(MISSING)     2d5h
  kubernetes-dashboar         kubernetes-dashboard-auth-66cf496dfc-4kjgx               100m (1%!)(MISSING)     250m (3%!)(MISSING)   200Mi (1%!)(MISSING)       400Mi (2%!)(MISSING)     2d5h
  kubernetes-dashboar         kubernetes-dashboard-kong-7696bb8c88-2jfx5               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d5h
  kubernetes-dashboar         kubernetes-dashboard-metrics-scraper-69cf85488f-v2cg7    100m (1%!)(MISSING)     250m (3%!)(MISSING)   200Mi (1%!)(MISSING)       400Mi (2%!)(MISSING)     2d5h
  kubernetes-dashboar         kubernetes-dashboard-web-6557fb6496-8zr5n                100m (1%!)(MISSING)     250m (3%!)(MISSING)   200Mi (1%!)(MISSING)       400Mi (2%!)(MISSING)     2d5h
  kubernetes-dashboard        dashboard-metrics-scraper-b5fc48f67-8dl49                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d5h
  kubernetes-dashboard        kubernetes-dashboard-779776cb65-wwxcr                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d5h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1250m (15%!)(MISSING)  1 (12%!)(MISSING)
  memory             1170Mi (7%!)(MISSING)  1770Mi (11%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[  +0.000597] FS-Cache: O-cookie d=00000000b45346cc{9P.session} n=00000000a853ff1a
[  +0.000489] FS-Cache: O-key=[10] '34323934393337343637'
[  +0.000405] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000480] FS-Cache: N-cookie d=00000000b45346cc{9P.session} n=00000000b4840c41
[  +0.000550] FS-Cache: N-key=[10] '34323934393337343637'
[  +0.500317] /sbin/ldconfig: 
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.151817] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.017369] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Kiev not found. Is the tzdata package installed?
[  +0.256940] Exception: 
[  +0.000007] Operation canceled @p9io.cpp:258 (AcceptAsync)

[  +0.007118] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.006279] Buffer I/O error on dev sdc, logical block 134184960, lost sync page write
[  +0.000538] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.000533] Aborting journal on device sdc-8.
[  +0.000301] Buffer I/O error on dev sdc, logical block 134184960, lost sync page write
[  +0.000420] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.000435] EXT4-fs error (device sdc): ext4_put_super:1196: comm Xwayland: Couldn't clean up the journal
[  +0.000494] EXT4-fs (sdc): Remounting filesystem read-only
[  +0.600986] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#319 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +1.179898] /sbin/ldconfig: 
[  +0.000010] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.055790] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.007770] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.003452] WSL (1) ERROR: ConfigMountFsTab:2579: Processing fstab with mount -a failed.
[  +0.017091] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.005652] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001945] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.073447] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Kiev not found. Is the tzdata package installed?
[  +0.077424] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000815] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.015058] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002607] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001457] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000632] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001208] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002882] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.147033] /sbin/ldconfig: 
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.032202] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.019304] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Kiev not found. Is the tzdata package installed?
[  +0.179105] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001081] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001003] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000942] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002133] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001219] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000956] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000970] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.137502] netlink: 'init': attribute type 4 has an invalid length.
[  +1.124269] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.


==> etcd [07920dc1e0b7] <==
{"level":"info","ts":"2024-05-17T07:51:55.448784Z","caller":"traceutil/trace.go:171","msg":"trace[171543701] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:12366; }","duration":"111.998905ms","start":"2024-05-17T07:51:55.336755Z","end":"2024-05-17T07:51:55.448754Z","steps":["trace[171543701] 'agreement among raft nodes before linearized reading'  (duration: 106.194041ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-17T07:51:55.448609Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.709544ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-17T07:51:55.44896Z","caller":"traceutil/trace.go:171","msg":"trace[1694757157] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12366; }","duration":"106.205142ms","start":"2024-05-17T07:51:55.342733Z","end":"2024-05-17T07:51:55.448938Z","steps":["trace[1694757157] 'agreement among raft nodes before linearized reading'  (duration: 100.231579ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-17T07:53:20.848951Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.518971ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029231128659156 > lease_revoke:<id:70cc8f85856a449a>","response":"size:29"}
{"level":"warn","ts":"2024-05-17T07:53:49.942849Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"204.366159ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" ","response":"range_response_count:1 size:123165"}
{"level":"info","ts":"2024-05-17T07:53:49.943027Z","caller":"traceutil/trace.go:171","msg":"trace[123144604] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:12455; }","duration":"204.586659ms","start":"2024-05-17T07:53:49.738398Z","end":"2024-05-17T07:53:49.942985Z","steps":["trace[123144604] 'range keys from bolt db'  (duration: 204.260459ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-17T07:53:50.450768Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"138.724565ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029231128659281 > lease_revoke:<id:70cc8f85856a450d>","response":"size:29"}
{"level":"info","ts":"2024-05-17T07:57:47.153784Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12406}
{"level":"info","ts":"2024-05-17T07:57:47.17584Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12406,"took":"21.744253ms","hash":880137941,"current-db-size-bytes":4034560,"current-db-size":"4.0 MB","current-db-size-in-use-bytes":1875968,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-05-17T07:57:47.176042Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":880137941,"revision":12406,"compact-revision":11630}
{"level":"info","ts":"2024-05-17T07:58:29.343433Z","caller":"traceutil/trace.go:171","msg":"trace[1084225889] transaction","detail":"{read_only:false; response_revision:12678; number_of_response:1; }","duration":"208.26882ms","start":"2024-05-17T07:58:29.135036Z","end":"2024-05-17T07:58:29.343305Z","steps":["trace[1084225889] 'process raft request'  (duration: 199.241719ms)"],"step_count":1}
{"level":"info","ts":"2024-05-17T07:59:04.169713Z","caller":"traceutil/trace.go:171","msg":"trace[46362580] transaction","detail":"{read_only:false; response_revision:12706; number_of_response:1; }","duration":"181.776124ms","start":"2024-05-17T07:59:03.987765Z","end":"2024-05-17T07:59:04.169541Z","steps":["trace[46362580] 'process raft request'  (duration: 111.386869ms)","trace[46362580] 'compare'  (duration: 69.805056ms)"],"step_count":2}
{"level":"info","ts":"2024-05-17T07:59:24.977641Z","caller":"traceutil/trace.go:171","msg":"trace[556735732] transaction","detail":"{read_only:false; response_revision:12722; number_of_response:1; }","duration":"256.201231ms","start":"2024-05-17T07:59:24.7214Z","end":"2024-05-17T07:59:24.977601Z","steps":["trace[556735732] 'process raft request'  (duration: 255.505419ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-17T07:59:25.414016Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"336.230878ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029231128660650 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc8f85856a4aa9>","response":"size:41"}
{"level":"warn","ts":"2024-05-17T07:59:25.415261Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-17T07:59:24.979332Z","time spent":"435.91288ms","remote":"127.0.0.1:58310","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2024-05-17T07:59:25.414726Z","caller":"traceutil/trace.go:171","msg":"trace[1688860555] linearizableReadLoop","detail":"{readStateIndex:15706; appliedIndex:15705; }","duration":"255.285914ms","start":"2024-05-17T07:59:25.159346Z","end":"2024-05-17T07:59:25.414632Z","steps":["trace[1688860555] 'read index received'  (duration: 125.602¬µs)","trace[1688860555] 'applied index is now lower than readState.Index'  (duration: 255.154912ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-17T07:59:25.534504Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"256.516437ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-17T07:59:25.535032Z","caller":"traceutil/trace.go:171","msg":"trace[684318967] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12722; }","duration":"375.62639ms","start":"2024-05-17T07:59:25.159305Z","end":"2024-05-17T07:59:25.534931Z","steps":["trace[684318967] 'agreement among raft nodes before linearized reading'  (duration: 256.309933ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-17T07:59:25.535306Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-17T07:59:25.159244Z","time spent":"375.945296ms","remote":"127.0.0.1:58286","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-05-17T07:59:25.938197Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"202.657863ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029231128660651 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:12715 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128029231128660649 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-05-17T07:59:25.938591Z","caller":"traceutil/trace.go:171","msg":"trace[438020548] linearizableReadLoop","detail":"{readStateIndex:15707; appliedIndex:15706; }","duration":"397.829592ms","start":"2024-05-17T07:59:25.540712Z","end":"2024-05-17T07:59:25.938542Z","steps":["trace[438020548] 'read index received'  (duration: 91.902562ms)","trace[438020548] 'applied index is now lower than readState.Index'  (duration: 305.904029ms)"],"step_count":2}
{"level":"info","ts":"2024-05-17T07:59:25.938915Z","caller":"traceutil/trace.go:171","msg":"trace[1991413644] transaction","detail":"{read_only:false; response_revision:12723; number_of_response:1; }","duration":"518.968182ms","start":"2024-05-17T07:59:25.419899Z","end":"2024-05-17T07:59:25.938867Z","steps":["trace[1991413644] 'process raft request'  (duration: 212.832648ms)","trace[1991413644] 'compare'  (duration: 201.91575ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-17T07:59:25.939391Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-17T07:59:25.41982Z","time spent":"519.217086ms","remote":"127.0.0.1:58310","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:12715 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128029231128660649 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2024-05-17T07:59:26.133825Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"394.268327ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configuration.konghq.com/kongplugins/\" range_end:\"/registry/configuration.konghq.com/kongplugins0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-17T07:59:26.134309Z","caller":"traceutil/trace.go:171","msg":"trace[1050970100] range","detail":"{range_begin:/registry/configuration.konghq.com/kongplugins/; range_end:/registry/configuration.konghq.com/kongplugins0; response_count:0; response_revision:12723; }","duration":"395.142043ms","start":"2024-05-17T07:59:25.73907Z","end":"2024-05-17T07:59:26.134212Z","steps":["trace[1050970100] 'agreement among raft nodes before linearized reading'  (duration: 200.594426ms)","trace[1050970100] 'count revisions from in-memory index tree'  (duration: 193.5884ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-17T07:59:26.134497Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-17T07:59:25.738978Z","time spent":"395.453549ms","remote":"127.0.0.1:59698","response type":"/etcdserverpb.KV/Range","request count":0,"request size":100,"response count":0,"response size":29,"request content":"key:\"/registry/configuration.konghq.com/kongplugins/\" range_end:\"/registry/configuration.konghq.com/kongplugins0\" count_only:true "}
{"level":"warn","ts":"2024-05-17T07:59:26.236009Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"695.217967ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-17T07:59:26.236158Z","caller":"traceutil/trace.go:171","msg":"trace[594495918] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12723; }","duration":"695.545673ms","start":"2024-05-17T07:59:25.54058Z","end":"2024-05-17T07:59:26.236126Z","steps":["trace[594495918] 'agreement among raft nodes before linearized reading'  (duration: 399.178616ms)","trace[594495918] 'range keys from in-memory index tree'  (duration: 296.053851ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-17T07:59:26.236239Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-17T07:59:25.54051Z","time spent":"695.692676ms","remote":"127.0.0.1:58284","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-05-17T07:59:26.23629Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"298.348593ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourcequotas/\" range_end:\"/registry/resourcequotas0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-17T07:59:26.236332Z","caller":"traceutil/trace.go:171","msg":"trace[458112547] range","detail":"{range_begin:/registry/resourcequotas/; range_end:/registry/resourcequotas0; response_count:0; response_revision:12723; }","duration":"298.535097ms","start":"2024-05-17T07:59:25.937786Z","end":"2024-05-17T07:59:26.236321Z","steps":["trace[458112547] 'count revisions from in-memory index tree'  (duration: 296.441759ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-17T07:59:26.236474Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"447.087882ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-17T07:59:26.236506Z","caller":"traceutil/trace.go:171","msg":"trace[1915122625] range","detail":"{range_begin:/registry/csidrivers/; range_end:/registry/csidrivers0; response_count:0; response_revision:12723; }","duration":"447.271085ms","start":"2024-05-17T07:59:25.789222Z","end":"2024-05-17T07:59:26.236494Z","steps":["trace[1915122625] 'agreement among raft nodes before linearized reading'  (duration: 150.622022ms)","trace[1915122625] 'count revisions from in-memory index tree'  (duration: 296.593061ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-17T07:59:26.236535Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-17T07:59:25.789109Z","time spent":"447.418087ms","remote":"127.0.0.1:58684","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":0,"response size":29,"request content":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true "}
{"level":"warn","ts":"2024-05-17T07:59:26.236547Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.231002ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2024-05-17T07:59:26.236577Z","caller":"traceutil/trace.go:171","msg":"trace[1011462545] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:12723; }","duration":"199.327503ms","start":"2024-05-17T07:59:26.037238Z","end":"2024-05-17T07:59:26.236566Z","steps":["trace[1011462545] 'range keys from in-memory index tree'  (duration: 199.044398ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-17T07:59:26.648524Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"215.855002ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029231128660660 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:12721 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-05-17T07:59:26.648894Z","caller":"traceutil/trace.go:171","msg":"trace[130915990] linearizableReadLoop","detail":"{readStateIndex:15708; appliedIndex:15707; }","duration":"229.120342ms","start":"2024-05-17T07:59:26.419711Z","end":"2024-05-17T07:59:26.648831Z","steps":["trace[130915990] 'read index received'  (duration: 13.067136ms)","trace[130915990] 'applied index is now lower than readState.Index'  (duration: 216.047005ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-17T07:59:26.650054Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"230.364764ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-05-17T07:59:26.65012Z","caller":"traceutil/trace.go:171","msg":"trace[154902746] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:0; response_revision:12724; }","duration":"230.484567ms","start":"2024-05-17T07:59:26.419623Z","end":"2024-05-17T07:59:26.650108Z","steps":["trace[154902746] 'agreement among raft nodes before linearized reading'  (duration: 229.369946ms)"],"step_count":1}
{"level":"info","ts":"2024-05-17T07:59:26.659085Z","caller":"traceutil/trace.go:171","msg":"trace[1576583113] transaction","detail":"{read_only:false; response_revision:12724; number_of_response:1; }","duration":"319.843581ms","start":"2024-05-17T07:59:26.337506Z","end":"2024-05-17T07:59:26.65905Z","steps":["trace[1576583113] 'process raft request'  (duration: 95.029318ms)","trace[1576583113] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 215.358293ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-17T07:59:26.659343Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-17T07:59:26.337474Z","time spent":"320.017284ms","remote":"127.0.0.1:58468","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:12721 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-05-17T07:59:38.999946Z","caller":"traceutil/trace.go:171","msg":"trace[306602695] transaction","detail":"{read_only:false; response_revision:12733; number_of_response:1; }","duration":"133.347587ms","start":"2024-05-17T07:59:38.866552Z","end":"2024-05-17T07:59:38.9999Z","steps":["trace[306602695] 'process raft request'  (duration: 132.97008ms)"],"step_count":1}
{"level":"info","ts":"2024-05-17T08:02:47.173572Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12645}
{"level":"info","ts":"2024-05-17T08:02:47.176507Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12645,"took":"2.689798ms","hash":843990758,"current-db-size-bytes":4034560,"current-db-size":"4.0 MB","current-db-size-in-use-bytes":2064384,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-05-17T08:02:47.176578Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":843990758,"revision":12645,"compact-revision":12406}
{"level":"info","ts":"2024-05-17T08:07:47.181966Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12884}
{"level":"info","ts":"2024-05-17T08:07:47.184778Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12884,"took":"2.549214ms","hash":762088389,"current-db-size-bytes":4034560,"current-db-size":"4.0 MB","current-db-size-in-use-bytes":2056192,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-05-17T08:07:47.184829Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":762088389,"revision":12884,"compact-revision":12645}
{"level":"info","ts":"2024-05-17T08:11:33.351807Z","caller":"traceutil/trace.go:171","msg":"trace[957113401] transaction","detail":"{read_only:false; response_revision:13303; number_of_response:1; }","duration":"162.562099ms","start":"2024-05-17T08:11:33.189216Z","end":"2024-05-17T08:11:33.351778Z","steps":["trace[957113401] 'process raft request'  (duration: 162.334398ms)"],"step_count":1}
{"level":"info","ts":"2024-05-17T08:12:47.189976Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13124}
{"level":"info","ts":"2024-05-17T08:12:47.192401Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":13124,"took":"2.083102ms","hash":4082441881,"current-db-size-bytes":4034560,"current-db-size":"4.0 MB","current-db-size-in-use-bytes":2052096,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-05-17T08:12:47.192449Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4082441881,"revision":13124,"compact-revision":12884}
{"level":"info","ts":"2024-05-17T08:17:47.20384Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13363}
{"level":"info","ts":"2024-05-17T08:17:47.206864Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":13363,"took":"2.615924ms","hash":2881741238,"current-db-size-bytes":4034560,"current-db-size":"4.0 MB","current-db-size-in-use-bytes":2043904,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-17T08:17:47.206918Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2881741238,"revision":13363,"compact-revision":13124}
{"level":"info","ts":"2024-05-17T08:22:19.439127Z","caller":"traceutil/trace.go:171","msg":"trace[1940585717] transaction","detail":"{read_only:false; response_revision:13820; number_of_response:1; }","duration":"130.892627ms","start":"2024-05-17T08:22:19.30819Z","end":"2024-05-17T08:22:19.439083Z","steps":["trace[1940585717] 'process raft request'  (duration: 130.520628ms)"],"step_count":1}
{"level":"info","ts":"2024-05-17T08:22:47.21942Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13604}
{"level":"info","ts":"2024-05-17T08:22:47.222049Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":13604,"took":"2.390431ms","hash":2479667143,"current-db-size-bytes":4034560,"current-db-size":"4.0 MB","current-db-size-in-use-bytes":2056192,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-05-17T08:22:47.222114Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2479667143,"revision":13604,"compact-revision":13363}


==> etcd [5239868149a7] <==
{"level":"info","ts":"2024-05-18T11:55:42.391032Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":30003}
{"level":"info","ts":"2024-05-18T11:55:42.391163Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":25003}
{"level":"info","ts":"2024-05-18T11:59:33.926771Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24126}
{"level":"info","ts":"2024-05-18T11:59:33.929937Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":24126,"took":"2.917512ms","hash":1676570866,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2076672,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-05-18T11:59:33.929989Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1676570866,"revision":24126,"compact-revision":23886}
{"level":"info","ts":"2024-05-18T12:04:33.936237Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24367}
{"level":"info","ts":"2024-05-18T12:04:33.939763Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":24367,"took":"3.293495ms","hash":1411667509,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2060288,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-05-18T12:04:33.939861Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1411667509,"revision":24367,"compact-revision":24126}
{"level":"info","ts":"2024-05-18T12:09:33.946293Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24607}
{"level":"info","ts":"2024-05-18T12:09:33.950464Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":24607,"took":"3.900302ms","hash":261782336,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2023424,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-18T12:09:33.950531Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":261782336,"revision":24607,"compact-revision":24367}
{"level":"info","ts":"2024-05-18T12:14:33.978862Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24848}
{"level":"info","ts":"2024-05-18T12:14:33.981093Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":24848,"took":"2.026302ms","hash":1787150839,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2220032,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-05-18T12:14:33.981133Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1787150839,"revision":24848,"compact-revision":24607}
{"level":"warn","ts":"2024-05-18T12:15:07.071531Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.805015ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-05-18T12:15:07.071663Z","caller":"traceutil/trace.go:171","msg":"trace[954585712] range","detail":"{range_begin:/registry/priorityclasses/; range_end:/registry/priorityclasses0; response_count:0; response_revision:25139; }","duration":"111.99007ms","start":"2024-05-18T12:15:06.959644Z","end":"2024-05-18T12:15:07.071635Z","steps":["trace[954585712] 'count revisions from in-memory index tree'  (duration: 111.631349ms)"],"step_count":1}
{"level":"info","ts":"2024-05-18T12:19:33.963683Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25113}
{"level":"info","ts":"2024-05-18T12:19:33.968887Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":25113,"took":"4.77828ms","hash":435712988,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2342912,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-05-18T12:19:33.969176Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":435712988,"revision":25113,"compact-revision":24848}
{"level":"info","ts":"2024-05-18T12:22:21.290141Z","caller":"traceutil/trace.go:171","msg":"trace[817808745] transaction","detail":"{read_only:false; response_revision:25487; number_of_response:1; }","duration":"101.415664ms","start":"2024-05-18T12:22:21.188692Z","end":"2024-05-18T12:22:21.290108Z","steps":["trace[817808745] 'process raft request'  (duration: 42.359408ms)","trace[817808745] 'compare'  (duration: 58.930669ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-18T12:23:46.217879Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.888029ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029253889201216 > lease_revoke:<id:70cc8f8ad20c93f8>","response":"size:30"}
{"level":"info","ts":"2024-05-18T12:24:33.970415Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25353}
{"level":"info","ts":"2024-05-18T12:24:33.974578Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":25353,"took":"3.835557ms","hash":10701433,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":1970176,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-18T12:24:33.974922Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":10701433,"revision":25353,"compact-revision":25113}
{"level":"info","ts":"2024-05-18T12:29:33.984239Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25593}
{"level":"info","ts":"2024-05-18T12:29:33.986928Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":25593,"took":"2.408122ms","hash":3197328379,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":1974272,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-18T12:29:33.986991Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3197328379,"revision":25593,"compact-revision":25353}
{"level":"info","ts":"2024-05-18T12:34:33.994439Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25834}
{"level":"info","ts":"2024-05-18T12:34:33.998083Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":25834,"took":"3.287399ms","hash":1649800790,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":1961984,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-18T12:34:33.998129Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1649800790,"revision":25834,"compact-revision":25593}
{"level":"info","ts":"2024-05-18T12:36:45.548957Z","caller":"traceutil/trace.go:171","msg":"trace[577052926] transaction","detail":"{read_only:false; response_revision:26179; number_of_response:1; }","duration":"140.721241ms","start":"2024-05-18T12:36:45.408215Z","end":"2024-05-18T12:36:45.548936Z","steps":["trace[577052926] 'process raft request'  (duration: 140.56644ms)"],"step_count":1}
{"level":"info","ts":"2024-05-18T12:39:34.004902Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26075}
{"level":"info","ts":"2024-05-18T12:39:34.00835Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":26075,"took":"3.115947ms","hash":152155780,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":1941504,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-05-18T12:39:34.008416Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":152155780,"revision":26075,"compact-revision":25834}
{"level":"info","ts":"2024-05-18T12:40:51.094104Z","caller":"traceutil/trace.go:171","msg":"trace[1738522710] transaction","detail":"{read_only:false; response_revision:26378; number_of_response:1; }","duration":"141.134449ms","start":"2024-05-18T12:40:50.952927Z","end":"2024-05-18T12:40:51.094062Z","steps":["trace[1738522710] 'process raft request'  (duration: 61.051427ms)","trace[1738522710] 'compare'  (duration: 79.89662ms)"],"step_count":2}
{"level":"info","ts":"2024-05-18T12:44:34.013097Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26314}
{"level":"info","ts":"2024-05-18T12:44:34.015684Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":26314,"took":"2.354504ms","hash":638005230,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":1970176,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-18T12:44:34.015738Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":638005230,"revision":26314,"compact-revision":26075}
{"level":"info","ts":"2024-05-18T12:49:06.314139Z","caller":"traceutil/trace.go:171","msg":"trace[1210690898] linearizableReadLoop","detail":"{readStateIndex:33269; appliedIndex:33268; }","duration":"210.038263ms","start":"2024-05-18T12:49:06.10397Z","end":"2024-05-18T12:49:06.314008Z","steps":["trace[1210690898] 'read index received'  (duration: 134.327968ms)","trace[1210690898] 'applied index is now lower than readState.Index'  (duration: 75.702095ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-18T12:49:06.314928Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"210.525764ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-05-18T12:49:06.315186Z","caller":"traceutil/trace.go:171","msg":"trace[445934633] range","detail":"{range_begin:/registry/storageclasses/; range_end:/registry/storageclasses0; response_count:0; response_revision:26792; }","duration":"211.170465ms","start":"2024-05-18T12:49:06.103933Z","end":"2024-05-18T12:49:06.315103Z","steps":["trace[445934633] 'agreement among raft nodes before linearized reading'  (duration: 210.357964ms)"],"step_count":1}
{"level":"info","ts":"2024-05-18T12:49:34.019936Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26555}
{"level":"info","ts":"2024-05-18T12:49:34.024138Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":26555,"took":"3.895023ms","hash":3094096255,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2027520,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-18T12:49:34.024266Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3094096255,"revision":26555,"compact-revision":26314}
{"level":"info","ts":"2024-05-18T12:51:18.821984Z","caller":"traceutil/trace.go:171","msg":"trace[2144807171] transaction","detail":"{read_only:false; response_revision:26899; number_of_response:1; }","duration":"122.473789ms","start":"2024-05-18T12:51:18.699491Z","end":"2024-05-18T12:51:18.821965Z","steps":["trace[2144807171] 'process raft request'  (duration: 122.252587ms)"],"step_count":1}
{"level":"info","ts":"2024-05-18T12:54:34.028375Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26816}
{"level":"info","ts":"2024-05-18T12:54:34.0309Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":26816,"took":"2.299707ms","hash":3649013146,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2007040,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-18T12:54:34.030954Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3649013146,"revision":26816,"compact-revision":26555}
{"level":"info","ts":"2024-05-18T12:59:34.030809Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27055}
{"level":"info","ts":"2024-05-18T12:59:34.033029Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":27055,"took":"2.051397ms","hash":68071827,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2101248,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-05-18T12:59:34.033079Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":68071827,"revision":27055,"compact-revision":26816}
{"level":"info","ts":"2024-05-18T13:04:34.100686Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27301}
{"level":"info","ts":"2024-05-18T13:04:34.103393Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":27301,"took":"2.513035ms","hash":883066830,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2142208,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-05-18T13:04:34.10344Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":883066830,"revision":27301,"compact-revision":27055}
{"level":"info","ts":"2024-05-18T13:09:34.10481Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27541}
{"level":"info","ts":"2024-05-18T13:09:34.107271Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":27541,"took":"2.295205ms","hash":1645449545,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2007040,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-18T13:09:34.107333Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1645449545,"revision":27541,"compact-revision":27301}
{"level":"info","ts":"2024-05-18T13:14:34.104619Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27780}
{"level":"info","ts":"2024-05-18T13:14:34.107385Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":27780,"took":"2.575705ms","hash":2634082021,"current-db-size-bytes":4661248,"current-db-size":"4.7 MB","current-db-size-in-use-bytes":2613248,"current-db-size-in-use":"2.6 MB"}
{"level":"info","ts":"2024-05-18T13:14:34.107425Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2634082021,"revision":27780,"compact-revision":27541}


==> kernel <==
 13:16:34 up  5:04,  0 users,  load average: 0.58, 0.29, 0.27
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [7f7b7e95626f] <==
E0518 08:29:45.364394       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 3698680f-8ab4-44ff-9d36-d95bc5ff8c52, UID in object meta: "
E0518 08:29:46.005912       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0518 08:29:46.005963       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0518 08:29:47.006267       1 handler_proxy.go:93] no RequestInfo found in the context
E0518 08:29:47.006317       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0518 08:29:47.006326       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0518 08:29:48.179892       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.106.9.118:443: connect: no route to host
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0518 08:29:48.179948       1 handler_proxy.go:93] no RequestInfo found in the context
I0518 08:29:48.179953       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0518 08:29:48.179993       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0518 08:29:48.186954       1 handler_proxy.go:93] no RequestInfo found in the context
E0518 08:29:48.187008       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0518 08:29:48.187093       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.9.118:443: connect: no route to host
W0518 08:29:49.181400       1 handler_proxy.go:93] no RequestInfo found in the context
E0518 08:29:49.181467       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0518 08:29:49.181476       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0518 08:29:49.181487       1 handler_proxy.go:93] no RequestInfo found in the context
E0518 08:29:49.181521       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0518 08:29:49.183018       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0518 08:29:51.299623       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.9.118:443: connect: no route to host
I0518 08:29:51.796950       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0518 08:29:51.796950       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0518 08:29:51.949031       1 controller.go:615] quota admission added evaluator for: endpoints
E0518 08:29:54.424934       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.9.118:443: connect: no route to host
I0518 08:29:54.449597       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0518 08:29:57.954750       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0518 09:00:21.920567       1 alloc.go:330] "allocated clusterIPs" service="default/first-app" clusterIPs={"IPv4":"10.96.65.104"}
I0518 11:51:47.259530       1 trace.go:236] Trace[711006368]: "Update" accept:application/json, */*,audit-id:b8300f12-e7eb-4917-afcd-de5c74fd9ba6,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-May-2024 11:51:46.457) (total time: 802ms):
Trace[711006368]: ["GuaranteedUpdate etcd3" audit-id:b8300f12-e7eb-4917-afcd-de5c74fd9ba6,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 801ms (11:51:46.457)
Trace[711006368]:  ---"Txn call completed" 801ms (11:51:47.259)]
Trace[711006368]: [802.048857ms] [802.048857ms] END
I0518 11:51:47.261596       1 trace.go:236] Trace[1182308396]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ed8cd269-65e5-4cd1-b523-f8b6c6e8c6e0,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (18-May-2024 11:51:46.592) (total time: 668ms):
Trace[1182308396]: ["GuaranteedUpdate etcd3" audit-id:ed8cd269-65e5-4cd1-b523-f8b6c6e8c6e0,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 668ms (11:51:46.592)
Trace[1182308396]:  ---"Txn call completed" 668ms (11:51:47.261)]
Trace[1182308396]: [668.914749ms] [668.914749ms] END
I0518 11:51:50.988259       1 trace.go:236] Trace[706990701]: "Update" accept:application/json, */*,audit-id:4827687f-9651-4d32-ad9a-8a7cf35cbb07,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-May-2024 11:51:49.263) (total time: 1724ms):
Trace[706990701]: ["GuaranteedUpdate etcd3" audit-id:4827687f-9651-4d32-ad9a-8a7cf35cbb07,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1724ms (11:51:49.263)
Trace[706990701]:  ---"Txn call completed" 1723ms (11:51:50.988)]
Trace[706990701]: [1.724507493s] [1.724507493s] END
I0518 11:51:52.090342       1 trace.go:236] Trace[650883355]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:58e62499-61b3-4c42-ad59-8633d0c466be,client:127.0.0.1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (18-May-2024 11:51:50.232) (total time: 1857ms):
Trace[650883355]: ["GuaranteedUpdate etcd3" audit-id:58e62499-61b3-4c42-ad59-8633d0c466be,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1857ms (11:51:50.232)
Trace[650883355]:  ---"Txn call completed" 1857ms (11:51:52.090)]
Trace[650883355]: [1.857793412s] [1.857793412s] END
I0518 11:51:52.356471       1 trace.go:236] Trace[1807736469]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:bdcdd896-cd66-4fe1-be88-99a18e6aecc9,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:POST (18-May-2024 11:51:51.415) (total time: 941ms):
Trace[1807736469]: ["Create etcd3" audit-id:bdcdd896-cd66-4fe1-be88-99a18e6aecc9,key:/events/kube-system/kube-apiserver-minikube.17d09305dc957df7,type:*core.Event,resource:events 940ms (11:51:51.416)
Trace[1807736469]:  ---"TransformToStorage succeeded" 674ms (11:51:52.090)
Trace[1807736469]:  ---"Txn call succeeded" 266ms (11:51:52.356)]
Trace[1807736469]: [941.115122ms] [941.115122ms] END
I0518 11:51:52.741906       1 trace.go:236] Trace[1082715593]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (18-May-2024 11:51:50.715) (total time: 2026ms):
Trace[1082715593]: ---"initial value restored" 1374ms (11:51:52.090)
Trace[1082715593]: ---"Transaction prepared" 521ms (11:51:52.612)
Trace[1082715593]: ---"Txn call completed" 129ms (11:51:52.741)
Trace[1082715593]: [2.026024355s] [2.026024355s] END
I0518 12:57:10.432113       1 alloc.go:330] "allocated clusterIPs" service="default/backend" clusterIPs={"IPv4":"10.110.6.88"}
I0518 13:11:42.027225       1 alloc.go:330] "allocated clusterIPs" service="default/backend" clusterIPs={"IPv4":"10.111.242.28"}
I0518 13:14:31.990496       1 alloc.go:330] "allocated clusterIPs" service="default/backend" clusterIPs={"IPv4":"10.109.172.8"}


==> kube-apiserver [f071cdf3a3b0] <==
I0517 07:47:49.734868       1 trace.go:236] Trace[2044272274]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9df1a1bc-ab1a-46b1-b2f7-e291cbb6fca1,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:POST (17-May-2024 07:47:49.219) (total time: 515ms):
Trace[2044272274]: ---"Write to database call failed" len:3213,err:nodes "minikube" already exists 221ms (07:47:49.734)
Trace[2044272274]: [515.603002ms] [515.603002ms] END
I0517 07:47:49.832691       1 trace.go:236] Trace[146847400]: "Delete" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5d42a91f-3f18-4d7c-b966-e4410d6bab0e,client:127.0.0.1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:DELETE (17-May-2024 07:47:49.318) (total time: 513ms):
Trace[146847400]: ---"Object deleted from database" 513ms (07:47:49.832)
Trace[146847400]: [513.609617ms] [513.609617ms] END
I0517 07:47:49.832839       1 trace.go:236] Trace[1695462537]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:82b35b3a-87c4-4e58-9565-3b7da2288d58,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:default,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:POST (17-May-2024 07:47:49.217) (total time: 615ms):
Trace[1695462537]: [615.31261ms] [615.31261ms] END
I0517 07:47:50.214015       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0517 07:47:52.848165       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0517 07:47:54.012280       1 controller.go:615] quota admission added evaluator for: endpoints
I0517 07:47:54.313317       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0517 07:47:54.429013       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
E0517 07:47:59.318105       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: net/http: TLS handshake timeout
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0517 07:47:59.433301       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
E0517 07:47:59.615861       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 2777677b-f8f3-4c4d-ab7b-0d913339e064, UID in object meta: "
E0517 07:48:00.330316       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0517 07:48:00.330352       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0517 07:48:01.018892       1 storage.go:475] Address {10.244.0.10  0xc00802bdc0 0xc003258230} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [] vs 10.244.0.10 (kubernetes-dashboar/kubernetes-dashboard-metrics-scraper-69cf85488f-v2cg7))
E0517 07:48:01.019541       1 storage.go:485] Failed to find a valid address, skipping subset: &{[{10.244.0.10  0xc00802bdc0 0xc003258230}] [] [{ 8000 TCP <nil>}]}
W0517 07:48:01.331944       1 handler_proxy.go:93] no RequestInfo found in the context
E0517 07:48:01.332008       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0517 07:48:01.332022       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0517 07:48:02.662307       1 storage.go:475] Address {10.244.0.4  0xc008b996b0 0xc003b133b0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.13}] vs 10.244.0.4 (kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-8dl49))
E0517 07:48:02.662360       1 storage.go:485] Failed to find a valid address, skipping subset: &{[{10.244.0.4  0xc008b996b0 0xc003b133b0}] [] [{ 8000 TCP <nil>}]}
E0517 07:48:04.440233       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
E0517 07:48:07.538698       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.9.118:443: connect: no route to host
I0517 07:48:07.547450       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0517 07:48:09.332001       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: net/http: TLS handshake timeout
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0517 07:48:09.332046       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0517 07:48:09.332107       1 handler_proxy.go:93] no RequestInfo found in the context
E0517 07:48:09.332169       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0517 07:48:09.333873       1 handler_proxy.go:93] no RequestInfo found in the context
E0517 07:48:09.333909       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0517 07:48:09.340783       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0517 07:48:10.334939       1 handler_proxy.go:93] no RequestInfo found in the context
W0517 07:48:10.334939       1 handler_proxy.go:93] no RequestInfo found in the context
E0517 07:48:10.335056       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0517 07:48:10.335066       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0517 07:48:10.335121       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0517 07:48:10.336471       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0517 07:48:10.652701       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.9.118:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.9.118:443: connect: no route to host
I0517 07:51:55.437694       1 trace.go:236] Trace[2083458015]: "Update" accept:application/json, */*,audit-id:19f5cc3a-518e-4295-8134-b3578435da5d,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (17-May-2024 07:51:54.934) (total time: 502ms):
Trace[2083458015]: ["GuaranteedUpdate etcd3" audit-id:19f5cc3a-518e-4295-8134-b3578435da5d,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 502ms (07:51:54.935)
Trace[2083458015]:  ---"Txn call completed" 500ms (07:51:55.437)]
Trace[2083458015]: [502.605283ms] [502.605283ms] END
I0517 07:58:29.433778       1 trace.go:236] Trace[1534967270]: "Update" accept:application/json, */*,audit-id:77f54fd8-a0d6-44a5-8229-1c5e208c1ec2,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (17-May-2024 07:58:28.751) (total time: 682ms):
Trace[1534967270]: ["GuaranteedUpdate etcd3" audit-id:77f54fd8-a0d6-44a5-8229-1c5e208c1ec2,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 680ms (07:58:28.753)
Trace[1534967270]:  ---"About to Encode" 380ms (07:58:29.134)
Trace[1534967270]:  ---"Txn call completed" 298ms (07:58:29.433)]
Trace[1534967270]: [682.215152ms] [682.215152ms] END
I0517 07:59:25.946525       1 trace.go:236] Trace[1171670314]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (17-May-2024 07:59:24.972) (total time: 974ms):
Trace[1171670314]: ---"Transaction prepared" 439ms (07:59:25.417)
Trace[1171670314]: ---"Txn call completed" 528ms (07:59:25.946)
Trace[1171670314]: [974.113409ms] [974.113409ms] END


==> kube-controller-manager [66482504396f] <==
I0518 11:39:23.851347       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="84.601¬µs"
I0518 11:39:24.119593       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="229.241593ms"
I0518 11:39:24.193664       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="74.006875ms"
I0518 11:39:24.193894       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="85.101¬µs"
I0518 11:39:24.193975       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="51.5¬µs"
I0518 11:39:24.194110       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="102.643138ms"
I0518 11:39:24.234121       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="39.935565ms"
I0518 11:39:24.234966       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="553.305¬µs"
I0518 11:39:24.310262       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="1.11091ms"
I0518 11:39:25.993001       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="67.681418ms"
I0518 11:39:25.993171       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="85.4¬µs"
I0518 11:39:26.051791       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="40.187667ms"
I0518 11:39:26.111429       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="59.588844ms"
I0518 11:39:26.111661       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="102.501¬µs"
I0518 11:39:26.120312       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="76.399997ms"
I0518 11:39:26.191966       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="71.573254ms"
I0518 11:39:26.192128       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="46.5¬µs"
I0518 11:39:26.201716       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="332.003¬µs"
I0518 11:39:28.244672       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="22.938196ms"
I0518 11:39:28.245942       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="190.305¬µs"
I0518 11:39:28.315162       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="58.979331ms"
I0518 11:39:28.354531       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="39.28092ms"
I0518 11:39:28.354635       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="28.701¬µs"
I0518 11:39:54.446157       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="52.4¬µs"
I0518 11:39:54.616155       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="41.101¬µs"
I0518 11:39:54.623571       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="51.5¬µs"
I0518 11:39:54.626836       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="28.401¬µs"
I0518 11:39:56.362756       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="44.6¬µs"
I0518 11:39:56.641707       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="48.5¬µs"
I0518 11:39:56.650388       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="45.701¬µs"
I0518 11:39:56.653451       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="43.701¬µs"
I0518 11:39:58.581752       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="28.8¬µs"
I0518 11:39:58.675037       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="30.8¬µs"
I0518 11:39:58.976676       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="31.899¬µs"
I0518 11:39:58.981808       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="28.8¬µs"
I0518 12:09:59.565169       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-5f4bd44fbf" duration="8.6¬µs"
I0518 12:09:59.565861       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/first-app-c7fbd4568" duration="7.1¬µs"
I0518 12:46:41.706325       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="63.175541ms"
I0518 12:46:41.726122       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="19.683606ms"
I0518 12:46:41.726259       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="56¬µs"
I0518 12:46:43.324890       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="4.5212ms"
I0518 12:46:43.324986       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="35.2¬µs"
I0518 13:10:46.184288       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="7.8¬µs"
I0518 13:11:42.065378       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="21.443544ms"
I0518 13:11:42.086444       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="21.026943ms"
I0518 13:11:42.086565       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="46.5¬µs"
I0518 13:11:42.866081       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="6.841214ms"
I0518 13:11:42.866175       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="27.4¬µs"
I0518 13:13:08.226971       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-64b97cd45c" duration="16.301¬µs"
I0518 13:14:32.027432       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="20.352932ms"
I0518 13:14:32.041290       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="13.796964ms"
I0518 13:14:32.053595       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="12.212198ms"
I0518 13:14:32.053679       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="28.601¬µs"
I0518 13:14:35.754405       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="63.002¬µs"
I0518 13:14:48.982323       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="50.3¬µs"
I0518 13:15:02.980533       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="40.801¬µs"
I0518 13:15:14.973951       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="77.604¬µs"
I0518 13:15:31.976034       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="65.002¬µs"
I0518 13:15:42.985460       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="69.801¬µs"
I0518 13:16:22.980175       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/second-app-deployment-686d5d6ff" duration="379.304¬µs"


==> kube-controller-manager [7ed72c6999f1] <==
I0517 07:48:07.530627       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0517 07:48:07.530759       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0517 07:48:07.531518       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0517 07:48:07.531565       1 shared_informer.go:320] Caches are synced for namespace
I0517 07:48:07.531616       1 shared_informer.go:320] Caches are synced for service account
I0517 07:48:07.531744       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0517 07:48:07.531905       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0517 07:48:07.533148       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0517 07:48:07.533832       1 shared_informer.go:320] Caches are synced for endpoint
I0517 07:48:07.533864       1 shared_informer.go:320] Caches are synced for node
I0517 07:48:07.533968       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0517 07:48:07.534054       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0517 07:48:07.534067       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0517 07:48:07.534074       1 shared_informer.go:320] Caches are synced for cidrallocator
I0517 07:48:07.534726       1 shared_informer.go:320] Caches are synced for cronjob
I0517 07:48:07.535658       1 shared_informer.go:320] Caches are synced for TTL
I0517 07:48:07.537651       1 shared_informer.go:320] Caches are synced for PVC protection
I0517 07:48:07.539456       1 shared_informer.go:320] Caches are synced for deployment
I0517 07:48:07.539474       1 shared_informer.go:320] Caches are synced for daemon sets
I0517 07:48:07.539860       1 shared_informer.go:320] Caches are synced for job
I0517 07:48:07.542220       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0517 07:48:07.542688       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="97.299¬µs"
I0517 07:48:07.542775       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-api-6b547b9c5d" duration="60.8¬µs"
I0517 07:48:07.542843       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-auth-66cf496dfc" duration="54.499¬µs"
I0517 07:48:07.542933       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-kong-7696bb8c88" duration="78.199¬µs"
I0517 07:48:07.543027       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-metrics-scraper-69cf85488f" duration="75.799¬µs"
I0517 07:48:07.543086       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-web-6557fb6496" duration="45.1¬µs"
I0517 07:48:07.543129       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="31.9¬µs"
I0517 07:48:07.543153       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="39.299¬µs"
I0517 07:48:07.612575       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0517 07:48:07.612789       1 shared_informer.go:320] Caches are synced for TTL after finished
I0517 07:48:07.638412       1 shared_informer.go:320] Caches are synced for ReplicationController
I0517 07:48:07.647348       1 shared_informer.go:320] Caches are synced for disruption
I0517 07:48:07.714494       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="172.192516ms"
I0517 07:48:07.714733       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="117.699¬µs"
I0517 07:48:07.715991       1 shared_informer.go:320] Caches are synced for attach detach
I0517 07:48:07.717231       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0517 07:48:07.732223       1 shared_informer.go:320] Caches are synced for taint
I0517 07:48:07.732407       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0517 07:48:07.732520       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0517 07:48:07.732624       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0517 07:48:07.859026       1 shared_informer.go:320] Caches are synced for resource quota
I0517 07:48:07.914282       1 shared_informer.go:320] Caches are synced for resource quota
I0517 07:48:08.214189       1 shared_informer.go:320] Caches are synced for garbage collector
I0517 07:48:08.214297       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0517 07:48:08.224194       1 shared_informer.go:320] Caches are synced for garbage collector
I0517 07:48:21.933233       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="314.768438ms"
I0517 07:48:22.015005       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="70.198¬µs"
I0517 07:48:43.303777       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-kong-7696bb8c88" duration="184.597¬µs"
I0517 07:48:44.438330       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-kong-7696bb8c88" duration="154.597¬µs"
I0517 07:48:51.086964       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-kong-7696bb8c88" duration="24.2489ms"
I0517 07:48:51.087162       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-kong-7696bb8c88" duration="109.698¬µs"
I0517 07:48:52.838825       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-web-6557fb6496" duration="95.799¬µs"
I0517 07:49:05.762008       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-web-6557fb6496" duration="182.251¬µs"
I0517 07:49:26.785443       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-web-6557fb6496" duration="71.718¬µs"
I0517 07:49:40.786999       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-web-6557fb6496" duration="103.439¬µs"
I0517 07:50:04.760439       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-web-6557fb6496" duration="54.399¬µs"
I0517 07:50:19.764119       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-web-6557fb6496" duration="154.199¬µs"
I0517 07:51:10.511658       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-web-6557fb6496" duration="14.912818ms"
I0517 07:51:10.511813       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboar/kubernetes-dashboard-web-6557fb6496" duration="67.501¬µs"


==> kube-proxy [35ade71b27d9] <==
I0518 08:29:40.100858       1 server_linux.go:69] "Using iptables proxy"
I0518 08:29:40.386805       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0518 08:29:40.598640       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0518 08:29:40.598744       1 server_linux.go:165] "Using iptables Proxier"
I0518 08:29:40.601896       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0518 08:29:40.601922       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0518 08:29:40.602801       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0518 08:29:40.603240       1 server.go:872] "Version info" version="v1.30.0"
I0518 08:29:40.603266       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0518 08:29:40.605649       1 config.go:192] "Starting service config controller"
I0518 08:29:40.605682       1 shared_informer.go:313] Waiting for caches to sync for service config
I0518 08:29:40.606538       1 config.go:319] "Starting node config controller"
I0518 08:29:40.606588       1 shared_informer.go:313] Waiting for caches to sync for node config
I0518 08:29:40.607356       1 config.go:101] "Starting endpoint slice config controller"
I0518 08:29:40.607424       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0518 08:29:40.705969       1 shared_informer.go:320] Caches are synced for service config
I0518 08:29:40.706776       1 shared_informer.go:320] Caches are synced for node config
I0518 08:29:40.707981       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [c262cca0473a] <==
I0517 07:47:58.936125       1 server_linux.go:69] "Using iptables proxy"
I0517 07:47:59.133448       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0517 07:47:59.818009       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0517 07:47:59.818160       1 server_linux.go:165] "Using iptables Proxier"
I0517 07:47:59.821123       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0517 07:47:59.821181       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0517 07:47:59.823426       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0517 07:47:59.826177       1 server.go:872] "Version info" version="v1.30.0"
I0517 07:47:59.828243       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0517 07:47:59.835100       1 config.go:192] "Starting service config controller"
I0517 07:47:59.835151       1 shared_informer.go:313] Waiting for caches to sync for service config
I0517 07:47:59.839609       1 config.go:101] "Starting endpoint slice config controller"
I0517 07:47:59.912891       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0517 07:47:59.839736       1 config.go:319] "Starting node config controller"
I0517 07:47:59.913016       1 shared_informer.go:313] Waiting for caches to sync for node config
I0517 07:48:00.013110       1 shared_informer.go:320] Caches are synced for node config
I0517 07:48:00.037055       1 shared_informer.go:320] Caches are synced for service config
I0517 07:48:00.113711       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [8f2dc4a98538] <==
I0518 08:29:33.532252       1 serving.go:380] Generated self-signed cert in-memory
W0518 08:29:34.986396       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0518 08:29:34.986482       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0518 08:29:34.986498       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0518 08:29:34.986506       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0518 08:29:35.091244       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0518 08:29:35.091279       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0518 08:29:35.103956       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0518 08:29:35.104062       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0518 08:29:35.104085       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0518 08:29:35.104173       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0518 08:29:35.379539       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [e149a9d3084c] <==
I0517 07:47:45.413376       1 serving.go:380] Generated self-signed cert in-memory
W0517 07:47:49.230534       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0517 07:47:49.230602       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0517 07:47:49.230619       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0517 07:47:49.230630       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0517 07:47:49.423179       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0517 07:47:49.423298       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0517 07:47:49.427174       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0517 07:47:49.433296       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0517 07:47:49.435015       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0517 07:47:49.435142       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0517 07:47:49.634052       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 18 12:46:41 minikube kubelet[1543]: E0518 12:46:41.708492    1543 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="99301de0-4bbb-4c47-a546-038be5b7b92b" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: E0518 12:46:41.708502    1543 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="69dd2e65-c12e-4b86-a0ad-b250745a066a" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: E0518 12:46:41.708511    1543 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="642cf1ec-c688-4187-ae53-0232f6def743" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: E0518 12:46:41.708521    1543 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8b55c861-9ee0-43df-8aed-027ed1494aff" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: E0518 12:46:41.708530    1543 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="161efb90-e3e3-46b3-ac28-75796a437da1" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: I0518 12:46:41.708585    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="99301de0-4bbb-4c47-a546-038be5b7b92b" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: I0518 12:46:41.708604    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="161efb90-e3e3-46b3-ac28-75796a437da1" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: I0518 12:46:41.708614    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="99301de0-4bbb-4c47-a546-038be5b7b92b" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: I0518 12:46:41.708620    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="642cf1ec-c688-4187-ae53-0232f6def743" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: I0518 12:46:41.708629    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="69dd2e65-c12e-4b86-a0ad-b250745a066a" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: I0518 12:46:41.708636    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="8a842921-aba7-48cb-a1ac-9086611047eb" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: I0518 12:46:41.708643    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="8b55c861-9ee0-43df-8aed-027ed1494aff" containerName="kub-first-app"
May 18 12:46:41 minikube kubelet[1543]: I0518 12:46:41.821781    1543 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gdpfk\" (UniqueName: \"kubernetes.io/projected/86eb963f-bd1d-43d2-932c-395046ec8697-kube-api-access-gdpfk\") pod \"second-app-deployment-64b97cd45c-fcrh9\" (UID: \"86eb963f-bd1d-43d2-932c-395046ec8697\") " pod="default/second-app-deployment-64b97cd45c-fcrh9"
May 18 12:46:43 minikube kubelet[1543]: I0518 12:46:43.320844    1543 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/second-app-deployment-64b97cd45c-fcrh9" podStartSLOduration=2.320820344 podStartE2EDuration="2.320820344s" podCreationTimestamp="2024-05-18 12:46:41 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-18 12:46:43.320331344 +0000 UTC m=+15433.666725441" watchObservedRunningTime="2024-05-18 12:46:43.320820344 +0000 UTC m=+15433.667214541"
May 18 13:11:16 minikube kubelet[1543]: I0518 13:11:16.690129    1543 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-gdpfk\" (UniqueName: \"kubernetes.io/projected/86eb963f-bd1d-43d2-932c-395046ec8697-kube-api-access-gdpfk\") pod \"86eb963f-bd1d-43d2-932c-395046ec8697\" (UID: \"86eb963f-bd1d-43d2-932c-395046ec8697\") "
May 18 13:11:16 minikube kubelet[1543]: I0518 13:11:16.691634    1543 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/86eb963f-bd1d-43d2-932c-395046ec8697-kube-api-access-gdpfk" (OuterVolumeSpecName: "kube-api-access-gdpfk") pod "86eb963f-bd1d-43d2-932c-395046ec8697" (UID: "86eb963f-bd1d-43d2-932c-395046ec8697"). InnerVolumeSpecName "kube-api-access-gdpfk". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 18 13:11:16 minikube kubelet[1543]: I0518 13:11:16.790900    1543 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-gdpfk\" (UniqueName: \"kubernetes.io/projected/86eb963f-bd1d-43d2-932c-395046ec8697-kube-api-access-gdpfk\") on node \"minikube\" DevicePath \"\""
May 18 13:11:17 minikube kubelet[1543]: I0518 13:11:17.516115    1543 scope.go:117] "RemoveContainer" containerID="eea5b164970efd0a16d4c0169e1f1a30566ed8b2c4845287b2770ca6e3f2c1f9"
May 18 13:11:17 minikube kubelet[1543]: I0518 13:11:17.985470    1543 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="86eb963f-bd1d-43d2-932c-395046ec8697" path="/var/lib/kubelet/pods/86eb963f-bd1d-43d2-932c-395046ec8697/volumes"
May 18 13:11:42 minikube kubelet[1543]: I0518 13:11:42.065633    1543 topology_manager.go:215] "Topology Admit Handler" podUID="2c9c8b7c-3f8b-4505-98a6-254fd9098534" podNamespace="default" podName="second-app-deployment-64b97cd45c-vthrv"
May 18 13:11:42 minikube kubelet[1543]: E0518 13:11:42.065749    1543 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="86eb963f-bd1d-43d2-932c-395046ec8697" containerName="second-node"
May 18 13:11:42 minikube kubelet[1543]: E0518 13:11:42.065767    1543 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="99301de0-4bbb-4c47-a546-038be5b7b92b" containerName="kub-first-app"
May 18 13:11:42 minikube kubelet[1543]: I0518 13:11:42.065818    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="86eb963f-bd1d-43d2-932c-395046ec8697" containerName="second-node"
May 18 13:11:42 minikube kubelet[1543]: I0518 13:11:42.065829    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="99301de0-4bbb-4c47-a546-038be5b7b92b" containerName="kub-first-app"
May 18 13:11:42 minikube kubelet[1543]: I0518 13:11:42.170136    1543 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-57xbc\" (UniqueName: \"kubernetes.io/projected/2c9c8b7c-3f8b-4505-98a6-254fd9098534-kube-api-access-57xbc\") pod \"second-app-deployment-64b97cd45c-vthrv\" (UID: \"2c9c8b7c-3f8b-4505-98a6-254fd9098534\") " pod="default/second-app-deployment-64b97cd45c-vthrv"
May 18 13:11:42 minikube kubelet[1543]: I0518 13:11:42.859381    1543 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/second-app-deployment-64b97cd45c-vthrv" podStartSLOduration=0.85936392 podStartE2EDuration="859.36392ms" podCreationTimestamp="2024-05-18 13:11:42 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-18 13:11:42.858764019 +0000 UTC m=+16933.149784815" watchObservedRunningTime="2024-05-18 13:11:42.85936392 +0000 UTC m=+16933.150384716"
May 18 13:13:38 minikube kubelet[1543]: I0518 13:13:38.696834    1543 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-57xbc\" (UniqueName: \"kubernetes.io/projected/2c9c8b7c-3f8b-4505-98a6-254fd9098534-kube-api-access-57xbc\") pod \"2c9c8b7c-3f8b-4505-98a6-254fd9098534\" (UID: \"2c9c8b7c-3f8b-4505-98a6-254fd9098534\") "
May 18 13:13:38 minikube kubelet[1543]: I0518 13:13:38.698270    1543 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/2c9c8b7c-3f8b-4505-98a6-254fd9098534-kube-api-access-57xbc" (OuterVolumeSpecName: "kube-api-access-57xbc") pod "2c9c8b7c-3f8b-4505-98a6-254fd9098534" (UID: "2c9c8b7c-3f8b-4505-98a6-254fd9098534"). InnerVolumeSpecName "kube-api-access-57xbc". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 18 13:13:38 minikube kubelet[1543]: I0518 13:13:38.797574    1543 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-57xbc\" (UniqueName: \"kubernetes.io/projected/2c9c8b7c-3f8b-4505-98a6-254fd9098534-kube-api-access-57xbc\") on node \"minikube\" DevicePath \"\""
May 18 13:13:39 minikube kubelet[1543]: I0518 13:13:39.099809    1543 scope.go:117] "RemoveContainer" containerID="9696d02d7d11422dd686f7f456e0dc053f8107b1068e3e4fdf58bb716daefb88"
May 18 13:13:39 minikube kubelet[1543]: I0518 13:13:39.114378    1543 scope.go:117] "RemoveContainer" containerID="9696d02d7d11422dd686f7f456e0dc053f8107b1068e3e4fdf58bb716daefb88"
May 18 13:13:39 minikube kubelet[1543]: E0518 13:13:39.115610    1543 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 9696d02d7d11422dd686f7f456e0dc053f8107b1068e3e4fdf58bb716daefb88" containerID="9696d02d7d11422dd686f7f456e0dc053f8107b1068e3e4fdf58bb716daefb88"
May 18 13:13:39 minikube kubelet[1543]: I0518 13:13:39.115650    1543 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"9696d02d7d11422dd686f7f456e0dc053f8107b1068e3e4fdf58bb716daefb88"} err="failed to get container status \"9696d02d7d11422dd686f7f456e0dc053f8107b1068e3e4fdf58bb716daefb88\": rpc error: code = Unknown desc = Error response from daemon: No such container: 9696d02d7d11422dd686f7f456e0dc053f8107b1068e3e4fdf58bb716daefb88"
May 18 13:13:39 minikube kubelet[1543]: I0518 13:13:39.971912    1543 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="2c9c8b7c-3f8b-4505-98a6-254fd9098534" path="/var/lib/kubelet/pods/2c9c8b7c-3f8b-4505-98a6-254fd9098534/volumes"
May 18 13:14:32 minikube kubelet[1543]: I0518 13:14:32.025394    1543 topology_manager.go:215] "Topology Admit Handler" podUID="fcde4377-c976-4c30-be9e-40492d52ba66" podNamespace="default" podName="second-app-deployment-686d5d6ff-54zsm"
May 18 13:14:32 minikube kubelet[1543]: E0518 13:14:32.025511    1543 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="2c9c8b7c-3f8b-4505-98a6-254fd9098534" containerName="second-node"
May 18 13:14:32 minikube kubelet[1543]: I0518 13:14:32.025560    1543 memory_manager.go:354] "RemoveStaleState removing state" podUID="2c9c8b7c-3f8b-4505-98a6-254fd9098534" containerName="second-node"
May 18 13:14:32 minikube kubelet[1543]: I0518 13:14:32.165150    1543 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7cs75\" (UniqueName: \"kubernetes.io/projected/fcde4377-c976-4c30-be9e-40492d52ba66-kube-api-access-7cs75\") pod \"second-app-deployment-686d5d6ff-54zsm\" (UID: \"fcde4377-c976-4c30-be9e-40492d52ba66\") " pod="default/second-app-deployment-686d5d6ff-54zsm"
May 18 13:14:35 minikube kubelet[1543]: E0518 13:14:35.114802    1543 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="academind/kub-first-app:2"
May 18 13:14:35 minikube kubelet[1543]: E0518 13:14:35.114860    1543 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="academind/kub-first-app:2"
May 18 13:14:35 minikube kubelet[1543]: E0518 13:14:35.114936    1543 kuberuntime_manager.go:1256] container &Container{Name:second-node,Image:academind/kub-first-app:2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7cs75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod second-app-deployment-686d5d6ff-54zsm_default(fcde4377-c976-4c30-be9e-40492d52ba66): ErrImagePull: Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 18 13:14:35 minikube kubelet[1543]: E0518 13:14:35.114957    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ErrImagePull: \"Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"
May 18 13:14:35 minikube kubelet[1543]: E0518 13:14:35.747409    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ImagePullBackOff: \"Back-off pulling image \\\"academind/kub-first-app:2\\\"\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"
May 18 13:14:51 minikube kubelet[1543]: E0518 13:14:51.600300    1543 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="academind/kub-first-app:2"
May 18 13:14:51 minikube kubelet[1543]: E0518 13:14:51.600371    1543 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="academind/kub-first-app:2"
May 18 13:14:51 minikube kubelet[1543]: E0518 13:14:51.600501    1543 kuberuntime_manager.go:1256] container &Container{Name:second-node,Image:academind/kub-first-app:2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7cs75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod second-app-deployment-686d5d6ff-54zsm_default(fcde4377-c976-4c30-be9e-40492d52ba66): ErrImagePull: Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 18 13:14:51 minikube kubelet[1543]: E0518 13:14:51.600534    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ErrImagePull: \"Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"
May 18 13:15:02 minikube kubelet[1543]: E0518 13:15:02.966358    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ImagePullBackOff: \"Back-off pulling image \\\"academind/kub-first-app:2\\\"\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"
May 18 13:15:17 minikube kubelet[1543]: E0518 13:15:17.507534    1543 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="academind/kub-first-app:2"
May 18 13:15:17 minikube kubelet[1543]: E0518 13:15:17.507596    1543 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="academind/kub-first-app:2"
May 18 13:15:17 minikube kubelet[1543]: E0518 13:15:17.507668    1543 kuberuntime_manager.go:1256] container &Container{Name:second-node,Image:academind/kub-first-app:2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7cs75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod second-app-deployment-686d5d6ff-54zsm_default(fcde4377-c976-4c30-be9e-40492d52ba66): ErrImagePull: Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 18 13:15:17 minikube kubelet[1543]: E0518 13:15:17.507700    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ErrImagePull: \"Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"
May 18 13:15:31 minikube kubelet[1543]: E0518 13:15:31.966184    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ImagePullBackOff: \"Back-off pulling image \\\"academind/kub-first-app:2\\\"\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"
May 18 13:15:42 minikube kubelet[1543]: E0518 13:15:42.965266    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ImagePullBackOff: \"Back-off pulling image \\\"academind/kub-first-app:2\\\"\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"
May 18 13:15:54 minikube kubelet[1543]: E0518 13:15:54.965758    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ImagePullBackOff: \"Back-off pulling image \\\"academind/kub-first-app:2\\\"\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"
May 18 13:16:10 minikube kubelet[1543]: E0518 13:16:10.869664    1543 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="academind/kub-first-app:2"
May 18 13:16:10 minikube kubelet[1543]: E0518 13:16:10.869785    1543 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="academind/kub-first-app:2"
May 18 13:16:10 minikube kubelet[1543]: E0518 13:16:10.869957    1543 kuberuntime_manager.go:1256] container &Container{Name:second-node,Image:academind/kub-first-app:2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7cs75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod second-app-deployment-686d5d6ff-54zsm_default(fcde4377-c976-4c30-be9e-40492d52ba66): ErrImagePull: Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 18 13:16:10 minikube kubelet[1543]: E0518 13:16:10.869991    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ErrImagePull: \"Error response from daemon: pull access denied for academind/kub-first-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"
May 18 13:16:22 minikube kubelet[1543]: E0518 13:16:22.969476    1543 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-node\" with ImagePullBackOff: \"Back-off pulling image \\\"academind/kub-first-app:2\\\"\"" pod="default/second-app-deployment-686d5d6ff-54zsm" podUID="fcde4377-c976-4c30-be9e-40492d52ba66"


==> kubernetes-dashboard [0a207c8e379a] <==
I0518 08:29:40.989851       1 main.go:40] "Starting Kubernetes Dashboard API" version="1.6.0"
I0518 08:29:40.990436       1 init.go:47] Using in-cluster config
I0518 08:29:41.006217       1 main.go:118] "Successful initial request to the apiserver" version="v1.30.0"
I0518 08:29:41.006845       1 client.go:268] Creating in-cluster Sidecar client
E0518 08:29:41.012426       1 manager.go:96] Metric client health check failed: the server is currently unable to handle the request (get services kubernetes-dashboard-metrics-scraper). Retrying in 30 seconds.
I0518 08:29:41.079990       1 main.go:95] "Listening and serving on" address="0.0.0.0:8000"
I0518 08:30:11.046334       1 manager.go:101] Successful request to sidecar


==> kubernetes-dashboard [147d79a6409f] <==
I0517 07:48:01.319821       1 main.go:34] "Starting Kubernetes Dashboard Auth" version="1.1.3"
I0517 07:48:01.319897       1 init.go:47] Using in-cluster config
I0517 07:48:01.320075       1 main.go:41] "Listening and serving insecurely on" address="0.0.0.0:8000"


==> kubernetes-dashboard [5edaeb583107] <==
I0517 07:51:10.139365       1 main.go:37] "Starting Kubernetes Dashboard Web" version="1.3.0"
I0517 07:51:10.139436       1 init.go:47] Using in-cluster config
I0517 07:51:10.139687       1 main.go:57] "Listening and serving insecurely on" address="0.0.0.0:8000"


==> kubernetes-dashboard [6115042b1c60] <==
I0517 08:17:01.365546       1 main.go:145] Database updated: 1 nodes, 15 pods
I0517 08:18:01.371814       1 main.go:145] Database updated: 1 nodes, 15 pods
I0517 08:19:01.373147       1 main.go:145] Database updated: 1 nodes, 15 pods
I0517 08:20:01.373984       1 main.go:145] Database updated: 1 nodes, 15 pods
I0517 08:21:01.368908       1 main.go:145] Database updated: 1 nodes, 15 pods
I0517 08:22:01.363740       1 main.go:145] Database updated: 1 nodes, 15 pods
I0517 08:23:01.382748       1 main.go:145] Database updated: 1 nodes, 15 pods
10.244.0.1 - - [17/May/2024:08:17:02 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:17:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:17:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:17:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:17:32 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:17:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:17:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:18:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:18:02 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:18:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:18:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:18:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:18:32 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:18:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:18:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:19:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:19:02 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:19:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:19:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:19:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:19:32 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:19:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:19:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:20:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:20:02 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:20:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:20:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:20:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:20:32 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:20:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:20:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:21:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:21:02 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:21:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:21:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:21:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:21:32 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:21:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:21:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:22:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:22:02 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:22:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:22:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:22:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:22:32 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:22:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:22:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:23:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:23:02 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [17/May/2024:08:23:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:23:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:23:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [17/May/2024:08:23:32 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"


==> kubernetes-dashboard [6863e3930a7a] <==
I0518 08:29:40.981240       1 main.go:37] "Starting Kubernetes Dashboard Web" version="1.3.0"
I0518 08:29:40.981817       1 init.go:47] Using in-cluster config
I0518 08:29:40.982077       1 main.go:57] "Listening and serving insecurely on" address="0.0.0.0:8000"


==> kubernetes-dashboard [6f665fe8c136] <==
I0518 08:29:40.981514       1 main.go:34] "Starting Kubernetes Dashboard Auth" version="1.1.3"
I0518 08:29:40.981601       1 init.go:47] Using in-cluster config
I0518 08:29:40.981736       1 main.go:41] "Listening and serving insecurely on" address="0.0.0.0:8000"


==> kubernetes-dashboard [933452e1ba3c] <==
2024/05/17 07:48:01 Starting overwatch
2024/05/17 07:48:01 Using namespace: kubernetes-dashboard
2024/05/17 07:48:01 Using in-cluster config to connect to apiserver
2024/05/17 07:48:01 Using secret token for csrf signing
2024/05/17 07:48:01 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/05/17 07:48:01 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/05/17 07:48:01 Successful initial request to the apiserver, version: v1.30.0
2024/05/17 07:48:01 Generating JWE encryption key
2024/05/17 07:48:01 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/05/17 07:48:01 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/05/17 07:48:02 Initializing JWE encryption key from synchronized object
2024/05/17 07:48:02 Creating in-cluster Sidecar client
2024/05/17 07:48:02 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2024/05/17 07:48:02 Serving insecurely on HTTP port: 9090
2024/05/17 07:48:32 Successful request to sidecar


==> kubernetes-dashboard [d7573ac3dddd] <==
I0518 13:10:40.290857       1 main.go:145] Database updated: 1 nodes, 16 pods
I0518 13:11:40.372676       1 main.go:145] Database updated: 1 nodes, 15 pods
I0518 13:12:40.308128       1 main.go:145] Database updated: 1 nodes, 15 pods
I0518 13:13:40.282641       1 main.go:145] Database updated: 1 nodes, 15 pods
I0518 13:14:40.278796       1 main.go:145] Database updated: 1 nodes, 15 pods
I0518 13:15:40.282543       1 main.go:145] Database updated: 1 nodes, 15 pods
10.244.0.1 - - [18/May/2024:13:09:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:10:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:10:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:10:19 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:10:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:10:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:10:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:10:49 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:10:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:11:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:11:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:11:19 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:11:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:11:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:11:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:11:49 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:11:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:12:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:12:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:12:19 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:12:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:12:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:12:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:12:49 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:12:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:13:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:13:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:13:19 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:13:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:13:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:13:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:13:49 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:13:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:14:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:14:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:14:19 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:14:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:14:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:14:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:14:49 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:14:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:15:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:15:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:15:19 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:15:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:15:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:15:41 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:15:49 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:15:51 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:16:01 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:16:11 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:16:19 +0000] "GET /healthz HTTP/1.1" 200 13 "" "dashboard/dashboard-api:1.6.0"
10.244.0.1 - - [18/May/2024:13:16:21 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"
10.244.0.1 - - [18/May/2024:13:16:31 +0000] "GET / HTTP/1.1" 200 6 "" "kube-probe/1.30"


==> kubernetes-dashboard [f2d0c41cc646] <==
2024/05/18 11:41:33 [2024-05-18T11:41:33Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:33 [2024-05-18T11:41:33Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:33 [2024-05-18T11:41:33Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:33 [2024-05-18T11:41:33Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:33 Getting list of all pet sets in the cluster
2024/05/18 11:41:33 [2024-05-18T11:41:33Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of namespaces
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all cron jobs in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all deployments in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all jobs in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all pods in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all replica sets in the cluster
2024/05/18 11:41:38 Getting pod metrics
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all replication controllers in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all pet sets in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of namespaces
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all cron jobs in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all jobs in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all pods in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all deployments in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all replication controllers in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all replica sets in the cluster
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/18 11:41:38 Getting list of all pet sets in the cluster
2024/05/18 11:41:38 Getting pod metrics
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/18 11:41:38 [2024-05-18T11:41:38Z] Outcoming response to 127.0.0.1 with 200 status code


==> kubernetes-dashboard [fa57b295d235] <==
I0517 07:48:00.717960       1 main.go:40] "Starting Kubernetes Dashboard API" version="1.6.0"
I0517 07:48:00.719452       1 init.go:47] Using in-cluster config
I0517 07:48:00.917296       1 main.go:118] "Successful initial request to the apiserver" version="v1.30.0"
I0517 07:48:00.917979       1 client.go:268] Creating in-cluster Sidecar client
E0517 07:48:01.038241       1 manager.go:96] Metric client health check failed: the server is currently unable to handle the request (get services kubernetes-dashboard-metrics-scraper). Retrying in 30 seconds.
I0517 07:48:01.215122       1 main.go:95] "Listening and serving on" address="0.0.0.0:8000"
I0517 07:48:31.050682       1 manager.go:101] Successful request to sidecar


==> storage-provisioner [06feb258ea14] <==
I0518 08:29:39.603225       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0518 08:29:49.613704       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout


==> storage-provisioner [dbf88e325ac8] <==
I0518 08:30:01.192115       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0518 08:30:01.201733       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0518 08:30:01.202077       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0518 08:30:18.604020       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0518 08:30:18.604177       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"075488d9-ce3e-426d-abbf-bed2b6a9cf23", APIVersion:"v1", ResourceVersion:"14095", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8f81e2be-2291-483d-a85a-ff99e1cba087 became leader
I0518 08:30:18.604227       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8f81e2be-2291-483d-a85a-ff99e1cba087!
I0518 08:30:18.706012       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8f81e2be-2291-483d-a85a-ff99e1cba087!

